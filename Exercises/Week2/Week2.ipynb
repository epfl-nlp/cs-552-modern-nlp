{"cells":[{"cell_type":"markdown","id":"3275ace2","metadata":{"id":"3275ace2"},"source":["# ðŸ“š  Exercise Session - Week 2\n","\n","Welcome to Week 2 exercise's session of CS552-Modern NLP!\n","\n","\n","> **What will be covered:**\n","1. [**TASK A:** N-gram Language Models](#ngram_lm)\n","    - [Unigram Language Model](#unigram_lm)\n","    - [Bi-gram Language Model](#bigram_lm)\n","    - [Tri-gram Language Model](#trigram_lm)\n","     \n","2. [**TASK B:** Neural Language Models](#neural_lm)\n","    - [Fixed-Window Neural Language Model](#fixed_window_lm)\n","    - [RNN-based Language Model](#rnn_lm)\n","\n","> **By the end of the session you will be able to:**\n","> - âœ…  Compute and interpret the perplexity of a language model\n","> - âœ…  Implement N-gram language models for N=1,2,3\n","> - âœ…  Implement, train, and evaluate a fixed window language model\n","> - âœ…  Evaluate an RNN language model\n","> - âœ…  Understand the advantages and disadvantages of each of the above models"]},{"cell_type":"code","source":["# Connect to the exercise folder if you're using Colab\n","# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"7BChR8cinbA8"},"id":"7BChR8cinbA8","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"98eb6b6c","metadata":{"id":"98eb6b6c"},"outputs":[],"source":["# install the libraries if needed using the requirements.txt file\n","\n","%cd PATH_TO_EXERCISE_FOLDER\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","id":"272c0a7c","metadata":{"id":"272c0a7c"},"source":["<a name=\"ngram_lm\"></a>\n","## 1. Task A: N-gram Language Models"]},{"cell_type":"markdown","id":"85c137c6","metadata":{"id":"85c137c6"},"source":["\n","In this exercise, we will better understand the functioning of different types of (non-neural) language modeling, namely,  Unigram LM, Bi-gram LM, and Tri-gram LM."]},{"cell_type":"markdown","id":"066c0427","metadata":{"id":"066c0427"},"source":["### 1.1 Unigram Language Model <a name=\"unigram_lm\"></a>\n","In the simple Unigram language model, we pick/generate next token independent of the previous token. In other words, during the generation, we pick the tokens according to the token probability. Therefore, for an arbitrary sequence $x_1x_2~...x_n$, its respective probability becomes:\n","$$p(x_1x_2~...x_n) = \\Pi_{i=1} ^n p(x_i)$$\n","Let's use an unsupervised dataset (raw corpus) to evaluate this model's perplexity. We use Huggingface's `datasets` library to download needed datasets.\n","\n","\n","Here we use the `Penn Treebank` dataset, featuring a million words of 1989 Wall Street Journal material. The rare words in this version are already replaced with `<unk>` token. The numbers are also replaced with a special token. This token replacement helps us to end up with a more reasonable vocabulary size to work with.\n"]},{"cell_type":"code","execution_count":null,"id":"69afb9de","metadata":{"scrolled":true,"id":"69afb9de"},"outputs":[],"source":["import torch\n","import datasets\n","import numpy as np\n","from datasets import load_dataset\n","\n","ptb_dataset = load_dataset(\"ptb_text_only\", split=\"train\", trust_remote_code=True)\n","\n","# splitting dataset in train/test (to be later used for language model evaluation)\n","ptb_dataset = ptb_dataset.train_test_split(test_size=0.2, seed=1)\n","ptb_train, ptb_test = ptb_dataset['train'], ptb_dataset['test']"]},{"cell_type":"markdown","id":"20589cd0","metadata":{"id":"20589cd0"},"source":["#### Let's have a look at a few samples of the training dataset (and also the structure of the dataset)"]},{"cell_type":"code","execution_count":null,"id":"b6ceb684","metadata":{"id":"b6ceb684"},"outputs":[],"source":["print(f\"{ptb_train[0]}\\n\\n{ptb_train[1]}\\n\\n{ptb_train[2]}\")"]},{"cell_type":"markdown","id":"07207b13","metadata":{"id":"07207b13"},"source":["During generation with a given language model, we often need to have a `<stop>` token in our vocabulary to terminate the generation of a given sentence/paragraph. In this dataset, every sample is a sentence, and the `<stop>` token should be added to the end of every sample (i.e., end of sentence).\n","\n","#### Create a new train/test dataset starting from `ptb_train` and `ptb_test` that has a `<stop>` at the end of each sentence. (Note: do not change the structure of the datasets objects, and just change the respective sentences as discussed).\n","Hint: use the `.map()` functionality of the `datasets` package (read more [here](https://huggingface.co/docs/datasets/process#map]))."]},{"cell_type":"code","execution_count":null,"id":"aa1b7b70","metadata":{"id":"aa1b7b70"},"outputs":[],"source":["def add_stop_token(input_sample: dict):\n","    '''\n","    args:\n","        input_sample: a dict representing a sample of the dataset. (look above for the dict struture)\n","    output:\n","        modified_sample: modified dict adding <stop> at the end of each sentence.\n","    '''\n","    # TODO: add `<stop>` token at the end of each sentence\n","    # YOUR CODE HERE\n","\n","    return modified_sample\n","\n","# TODO: apply `add_stop_token` function in-place on ptb_train and ptb_test datasets\n","ptb_train = ...\n","ptb_test = ..."]},{"cell_type":"markdown","id":"2d338821","metadata":{"id":"2d338821"},"source":["For both `ptb_train` and `ptb_test` datasets, filter out every sample that has less than 3 tokens. it will help remove very short sentences that are not very helpful for training/evaluating a langugage model.\n","\n","Hint: use `.filter()` functionality of the `datasets` package (read more [here](https://huggingface.co/docs/datasets/process#select-and-filter))."]},{"cell_type":"code","execution_count":null,"id":"9c0222f5","metadata":{"id":"9c0222f5"},"outputs":[],"source":["# YOUR CODE HERE\n","ptb_train = ...\n","ptb_test = ..."]},{"cell_type":"markdown","id":"2733f77b","metadata":{"id":"2733f77b"},"source":["#### Print the 10 most frequent tokens in `ptb_train` dataset.\n","\n","> **Observation**: Can you spot the token used to replace the numbers in this dataset? How are rare tokens replaced in this dataset?"]},{"cell_type":"code","execution_count":null,"id":"28e93fb6","metadata":{"id":"28e93fb6"},"outputs":[],"source":["from collections import Counter\n","\n","# YOUR CODE HERE\n","..."]},{"cell_type":"markdown","id":"a6eaec09","metadata":{"id":"a6eaec09"},"source":["#### Now let's create a dictionary of the word probabilites (in the format of `{word: Prob(word)}`in the following function. We will use these probabilities to estimate sequence probabilities for a given sequence, as mentioned above."]},{"cell_type":"code","execution_count":null,"id":"0e015a74","metadata":{"id":"0e015a74"},"outputs":[],"source":["from collections import defaultdict, Counter\n","\n","def get_word_probability_dict(train_dataset: datasets.arrow_dataset.Dataset):\n","    '''\n","    args:\n","        train_dataset: a Dataset object that can be iterated to get all the sentences\n","    output:\n","        word_prob_dict: a dictionary containing the word probabilities (and outputing zero for non-seen tokens)\n","    '''\n","    # TODO: get word probabilities across all available tokens in the train_dataset\n","    # YOUR CODE HERE\n","    word_prob_dict = ...\n","\n","    return word_prob_dict\n","\n","word_prob_dict = get_word_probability_dict(ptb_train)"]},{"cell_type":"markdown","id":"766d403e","metadata":{"id":"766d403e"},"source":["Let's also get a sense of how high the top-k probabilities are:"]},{"cell_type":"code","execution_count":null,"id":"37d958f6","metadata":{"scrolled":true,"id":"37d958f6"},"outputs":[],"source":["sorted(word_prob_dict.items(), key=lambda item: item[1], reverse=True)[:20]"]},{"cell_type":"markdown","id":"e1b408c2","metadata":{"id":"e1b408c2"},"source":["#### Now let's analyze the Unigram language model for different sequences. We first create a function that can output the probability for a given string."]},{"cell_type":"code","execution_count":null,"id":"fc167228","metadata":{"id":"fc167228"},"outputs":[],"source":["def unigram_lm_seq_probability(input_sentence: str,\n","                               word_prob_dict: dict):\n","    '''\n","    args:\n","        input_sentence: The input sequence string. Here we assume\n","        word_prob_dict: A dictionary containing the probability for a given token\n","    output:\n","        probability: The probability of the input_sentence according to the Unigram language model\n","    '''\n","    # TODO: get sequence probability using unigram probabilities\n","    # YOUR CODE HERE\n","    probability = ...\n","    return probability"]},{"cell_type":"markdown","id":"3427e046","metadata":{"id":"3427e046"},"source":["#### Let's investigate a major issue with Unigram language model. What are the probabilities for the two following sequences?\n","- the the the the \\<stop>\n","- i love computer science \\<stop>\n","\n","DIscussion: How can we avoid having large probability values for sequences like `the the the <stop>`"]},{"cell_type":"code","execution_count":null,"id":"2c753d2d","metadata":{"id":"2c753d2d"},"outputs":[],"source":["seq1 = \"the the the the <stop>\"\n","seq2 = \"i love computer science <stop>\"\n","\n","prob_seq1 = unigram_lm_seq_probability(seq1, word_prob_dict)\n","prob_seq2 = unigram_lm_seq_probability(seq2, word_prob_dict)\n","print(f\"probability for seq1 is {prob_seq1}, and for seq2 is {prob_seq2}\")"]},{"cell_type":"markdown","id":"06747969","metadata":{"id":"06747969"},"source":["#### Now let's formally evaluate the Unigram model in terms of perplexity. We first compute the entropy as the average negative log-likelihood:\n","$$H(W_{test}âˆ£M)= \\frac{1}{|W_{test}|} \\sum_{w\\in W_{test}} âˆ’log_2P(wâˆ£M)$$\n",", where $W_{test}$ is the input sequence and M is the Unigram language model. (note that the logarithm is in base 2).\n","\n","#### We provide **`get_unigram_lm_entropy`** function to perform the entropy calculation."]},{"cell_type":"code","execution_count":null,"id":"aa48300f","metadata":{"id":"aa48300f"},"outputs":[],"source":["def get_unigram_lm_entropy(input_sentence: str,\n","                           word_prob_dict: dict):\n","    '''\n","    args:\n","        input_sentence: the input string that we would like to have its respective entropy value.\n","        word_prob_dict: A dictionary containing the probability for a given token\n","    output:\n","        entropy: entropy value as defined above\n","    '''\n","    entropy = np.mean([-np.log2(max(word_prob_dict[word], 1e-6)) for word in input_sentence.split()]) # prevent divide-by-zero error\n","    return entropy"]},{"cell_type":"markdown","id":"b07afa63","metadata":{"id":"b07afa63"},"source":["Now compute the average entropy for all the sentences in the `ptb_test` given above function, and then compute the average entropy. Then compute the perplexity as $2^{\\bar{H}}$, where $\\bar{H}$ is the average entropy over the test dataset."]},{"cell_type":"code","execution_count":null,"id":"ac66be2c","metadata":{"id":"ac66be2c"},"outputs":[],"source":["def get_unigram_lm_perplexity(test_dataset: datasets.arrow_dataset.Dataset,\n","                              word_prob_dict: dict):\n","    '''\n","    args:\n","        test_dataset: the test dataset samples are used to compute the perplexity for the Unigram LM.\n","        word_prob_dict: A dictionary containing the probability for a given token\n","    output:\n","        perplexity: entropy value as defined above\n","    '''\n","    # TODO: Compute perplexity on the test dataset with unigram probabilities\n","    # YOUR CODE HERE\n","    ...\n","    return perplexity\n","\n","unigram_lm_perplexity = get_unigram_lm_perplexity(ptb_test, word_prob_dict)\n","print(f\"The perplexity for the Unigram language model is {unigram_lm_perplexity}\")"]},{"cell_type":"markdown","id":"4a3cec0a","metadata":{"id":"4a3cec0a"},"source":["As discussed in the lectures, the models with lower perplexities are desired; however, we should be careful when comparing language models with different vocabualry sizes.\n","#### In the `ptb_train` dataset, replace every token that is appearing less than 10 times with the `<unk>` token. (Note: the same token replacement should be done for the test dataset). What is the Unigram language model perplexity for the new dataset?\n","Discussion: What would happen to the vocabulary size and perplexity as we increase the rare token threshold to higher values? (instead of 10 here)"]},{"cell_type":"code","execution_count":null,"id":"3a14c853","metadata":{"id":"3a14c853"},"outputs":[],"source":["def remove_rare_token(train_dataset: datasets.arrow_dataset.Dataset,\n","                      test_dataset: datasets.arrow_dataset.Dataset,\n","                      rare_token_threshold: int):\n","    '''\n","    Note that the tokens that are considered rare here, are identified based on the train_dataset, so that\n","    we have the same token mapping (to <unk>) for both the train and test datasets.\n","    args:\n","        train_dataset: the input dataset where its rare tokens has to be replaced with <unk> token.\n","        rare_token_threshold: every word that is appearing less than this threshold in the train dataset will\n","                              be replace with the <unk> token\n","    output:\n","        cleaned_train_dataset: the cleaned train dataset where rare tokens are replace with <unk> token.\n","        cleaned_test_dataset: the cleaned test dataset where rare tokens are replace with <unk> token.\n","    '''\n","    all_tokens = \" \".join([i[\"sentence\"] for i in train_dataset]).split()\n","    word_frequency_dict = Counter(all_tokens)\n","    def rare_removal(input_sample):\n","        modified_sample = input_sample\n","        # TODO: replace rare tokens with `<unk>`\n","        # YOUR CODE HERE\n","        modified_sample['sentence'] = ...\n","        return modified_sample\n","\n","    cleaned_train_dataset = train_dataset.map(rare_removal)\n","    cleaned_test_dataset = test_dataset.map(rare_removal)\n","\n","    return cleaned_train_dataset, cleaned_test_dataset\n","\n","cleaned_train_dataset, cleaned_test_dataset = remove_rare_token(train_dataset=ptb_train,\n","                                                                test_dataset=ptb_test,\n","                                                                rare_token_threshold=10)\n"]},{"cell_type":"markdown","id":"6f09a7d3","metadata":{"id":"6f09a7d3"},"source":["#### Compute the test perplexity (`cleaned_test_dataset`) given token probabilities on a train dataset (`cleaned_train_dataset`)"]},{"cell_type":"code","execution_count":null,"id":"d2083f50","metadata":{"id":"d2083f50"},"outputs":[],"source":["cleaned_word_prob_dict = get_word_probability_dict(cleaned_train_dataset)\n","cleaned_unigram_lm_perplexity = get_unigram_lm_perplexity(cleaned_test_dataset, cleaned_word_prob_dict)\n","\n","print(\"The perplexity for the Unigram language model after replacing rare tokens is \",\n","      cleaned_unigram_lm_perplexity)"]},{"cell_type":"markdown","id":"4abe9eac","metadata":{"id":"4abe9eac"},"source":["## 1.2 Bi-gram Language Model <a name='bigram_lm'></a>\n","In the Bi-gram language model, we pick/generate next token conditioned only on the previous token. Therefore, for an arbitrary sequence $x_1x_2~...x_n$, its respective probability becomes:\n","$$p(x_1x_2~...x_n) = p(x_1) ~\\Pi_{i=2} ^n p(x_i|x_{i-1})$$\n","Let's use the same dataset (`Penn Treebank`) to evaluate this model's perplexity. (We use the dataset that already has the `<stop>` token at the end).\n","\n","We estimate $p(x_i|x_{i-1})$ as the $\\frac{count(x_{i-1},~x_i)}{count(x_{i-1})}$ according to the training dataset frequencies."]},{"cell_type":"code","execution_count":null,"id":"f96966fc","metadata":{"id":"f96966fc"},"outputs":[],"source":["def get_first_order_conditional_probabilities(train_dataset: datasets.arrow_dataset.Dataset):\n","    '''\n","    In this function the conditional probabilities have to be computed based train_dataset. The output of the\n","    function is a dictionary having keys like (x_{i-1}, x_i) as a tuple and the value being p(x_i|x_{i-1}).\n","    args:\n","        train_dataset: a Dataset object that can be iterated to get all the sentences\n","    output:\n","        word_prob_dict:\n","        first_order_condition_prob: a dictionary having containing the first order conditional probabilities\n","                                    as discussed above.\n","        word_prob_dict: a dictionary containing the word probabilities\n","    '''\n","    first_order_condition_prob = defaultdict(float) # in order to get zeroes\n","    # let's first get the word frequencies (later used for computation of conditional probabilities)\n","    word_prob_dict = get_word_probability_dict(train_dataset)\n","\n","    all_tokens = \" \".join([i[\"sentence\"] for i in train_dataset]).split()\n","    word_frequency_dict = Counter(all_tokens)\n","\n","    # TODO: Get all bigrams and bigram probabilities\n","    all_bigrams = []\n","    for sample in train_dataset:\n","        # YOUR CODE HERE\n","        ...\n","\n","    bigram_frequency_dict = Counter(all_bigrams)\n","    # YOUR CODE HERE\n","    first_order_condition_prob = ...\n","\n","    return word_prob_dict, first_order_condition_prob\n","\n","word_prob_dict, first_order_condition_prob = get_first_order_conditional_probabilities(ptb_train)"]},{"cell_type":"markdown","id":"c89cb406","metadata":{"id":"c89cb406"},"source":["#### Now let's analyze the Bi-gram language model for different sequences. We first create a function that can output the probability for a given string."]},{"cell_type":"code","execution_count":null,"id":"e459cdc4","metadata":{"id":"e459cdc4"},"outputs":[],"source":["def bigram_lm_seq_probability(input_sentence: str,\n","                              word_prob_dict: dict,\n","                              first_order_condition_prob: dict):\n","    '''\n","    args:\n","        input_sentence: The input sequence string. Here we assume\n","        word_prob_dict: a dictionary containing the word probabilities\n","        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n","                                    as discussed in the previous function.\n","    output:\n","        probability: The probability of the input_sentence according to the Bi-gram language model\n","    '''\n","    # YOUR CODE HERE\n","\n","    return probability"]},{"cell_type":"markdown","id":"a87509ef","metadata":{"id":"a87509ef"},"source":["Let's investigate a major issue with higher order language models.\n","#### Compute the probabilities for all the sequences in `ptb_test` dataset, and compute the minimum value among these probablities. What would be the perplexity for the dataset given these values?\n","Discussion: How can we avoid this **overfitting** to train dataset?"]},{"cell_type":"code","execution_count":null,"id":"3d9a0460","metadata":{"id":"3d9a0460"},"outputs":[],"source":["# TODO: get bigram probabilities on test dataset\n","# YOUR CODE HERE\n","bigram_test_probabilities = ...\n","\n","print(f\"{bigram_test_probabilities.count(0)/len(ptb_test)*100}% of samples in the test set have zero probability!\")"]},{"cell_type":"markdown","id":"9ced914f","metadata":{"id":"9ced914f"},"source":["### Smoothing\n","As we saw above, due to having new pair of consecutive words in the test dataset, we might have zero probabilities for some sequences. Therefore, as discussed in the lectures, in order to have a meaningful perplexity for N-gram language models, we need to smooth the probabilities to have non-zero values for non-seen sequences. In this exercise, we use Laplace smoothing as defined below:\n","$$P(x_i|x_{i-1}) = \\frac{count(x_{i-1},~x_i) + \\alpha}{count(x_{i-1}) + \\alpha ~|V|}$$\n",", where $\\alpha$ is the smoothing parameter, and $|V|$ is the (train dataset) vocabulary size.\n","\n","#### Let's recompute the conditional probabilities using Laplace smoothing."]},{"cell_type":"code","execution_count":null,"id":"5d808944","metadata":{"id":"5d808944"},"outputs":[],"source":["def get_smoothed_first_order_conditional_probabilities(train_dataset: datasets.arrow_dataset.Dataset,\n","                                                       smoothing_alpha: float):\n","    '''\n","    In this function the conditional probabilities have to be computed based on train_dataset. The output\n","    of the function is a dictionary having keys like (x_{i-1}, x_i) as a tuple and the\n","    value being p(x_i|x_{i-1}).\n","    args:\n","        train_dataset: a Dataset object that can be iterated to get all the sentences\n","        smoothing_alpha: The alpha parameter used in the Laplace smoothing.\n","    output:\n","        word_prob_dict: a dictionary containing the word probabilities\n","        first_order_condition_prob: a dictionary containing the smoothed first order\n","                                    conditional probabilities as discussed above.\n","    '''\n","    first_order_condition_prob = defaultdict(float)  # Note that we shouldn't get zeros for unseen events.\n","    # let's first get the word probabilities (later used for computation of conditional probabilities)\n","    word_prob_dict = get_word_probability_dict(train_dataset)\n","\n","    all_tokens = \" \".join([i[\"sentence\"] for i in train_dataset]).split()\n","    word_frequency_dict = Counter(all_tokens)\n","    vocab_size = len(word_frequency_dict)\n","\n","    all_bigrams = []\n","    for sample in train_dataset:\n","        # TODO: get bigram frequencies\n","        # YOUR CODE HERE\n","        ...\n","    bigram_frequency_dict = Counter(all_bigrams)\n","\n","    # TODO: get bigram probabilities\n","    # YOUR CODE HERE\n","    first_order_condition_prob = ...\n","\n","    return word_prob_dict, first_order_condition_prob"]},{"cell_type":"code","execution_count":null,"id":"3008240c","metadata":{"id":"3008240c"},"outputs":[],"source":["def smoothed_bigram_lm_seq_probability(input_sentence: str,\n","                                       word_prob_dict: dict,\n","                                       word_frequency_dict: dict,\n","                                       first_order_condition_prob: dict,\n","                                       smoothing_alpha: float):\n","    '''\n","    args:\n","        input_sentence: The input sequence string. Here we assume\n","        word_prob_dict: a dictionary containing the word probabilities\n","        word_frequency_dict: a dictionary containing the frequency for every word in vocabulary\n","        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n","                                    as discussed in the previous function.\n","        smoothing_alpha: The alpha parameter used in the Laplace smoothing.\n","    output:\n","        probability: The probability of the input_sentence according to the Bi-gram language model\n","    '''\n","    vocab_size = len(word_prob_dict)\n","    token_list = input_sentence.split()\n","    bigram_list = [(s1, s2) for s1, s2 in zip(token_list, token_list[1:])]\n","\n","    # TODO: compute sentence probability\n","    # YOUR CODE HERE\n","    probability = ...\n","\n","    return probability"]},{"cell_type":"markdown","id":"4cd77e6c","metadata":{"id":"4cd77e6c"},"source":["#### Assuming $\\alpha=0.01$ for the smoothing, use the previous function and `bigram_lm_seq_probability` to compute the sequence probabilities for all the sentences in the `ptb_test` dataset."]},{"cell_type":"code","execution_count":null,"id":"e2c76c94","metadata":{"id":"e2c76c94"},"outputs":[],"source":["smoothed_bigram_test_probabilities = []\n","\n","word_frequency_dict = Counter(\" \".join([i[\"sentence\"] for i in ptb_train]).split())\n","(word_prob_dict, smoothed_first_order_condition_prob) = get_smoothed_first_order_conditional_probabilities(ptb_train, 0.01)\n","\n","# TODO: get smoothed bigram probabilities\n","# YOUR CODE HERE\n","smoothed_bigram_test_probabilities =...\n","\n","print(f\"{smoothed_bigram_test_probabilities.count(0)/len(ptb_test)*100}% of samples in the test set have zero probability!\")"]},{"cell_type":"markdown","id":"8f8b38b8","metadata":{"id":"8f8b38b8"},"source":["If the perplexity for a given sequence is computed as below, compute the Bigram language model perplexity over `ptb_test` dataset over all the sentences ($\\alpha=0.01)$:\n","$$Perplexity(x_1x_2...x_n) = p(x_1x_2...x_n)^{-1/n}$$\n",", where $p(x_1x_2...x_n)$ is the probability assigned to $x_1x_2...x_n$ sequence by the language model."]},{"cell_type":"code","execution_count":null,"id":"7e3665f1","metadata":{"id":"7e3665f1"},"outputs":[],"source":["bigram_lm_perplexity = -1\n","\n","log_perplex_list = []\n","for idx in range(len(ptb_test)):\n","    # TODO: get bigram perplexities\n","    # YOUR CODE HERE\n","    ...\n","bigram_lm_perplexity = 2**np.mean(log_perplex_list)\n","\n","print(f\"Bigram language model perplexity is {bigram_lm_perplexity}\")"]},{"cell_type":"markdown","id":"edd753ef","metadata":{"id":"edd753ef"},"source":["Repeat the same steps but for `cleaned_train_dataset` and `cleaned_test_dataset` datasets where rare tokens (with frequency less than 10) are replaced with `<unk>` token. Do we have a better or a worse perplexity compared to the previous computed perplexity?"]},{"cell_type":"code","execution_count":null,"id":"92b98f8b","metadata":{"id":"92b98f8b"},"outputs":[],"source":["cleaned_word_frequency_dict = Counter(\" \".join([i[\"sentence\"] for i in cleaned_train_dataset]).split())\n","(cleaned_word_prob_dict,\n"," cleaned_smoothed_first_order_condition_prob) = get_smoothed_first_order_conditional_probabilities(cleaned_train_dataset, 0.01)\n","\n","# TODO: get smoothed bigram probabilities on the test set\n","# YOUR CODE HERE\n","cleaned_smoothed_bigram_test_probabilities = ...\n","\n","log_perplex_list = []\n","for idx in range(len(cleaned_test_dataset)):\n","    # TODO: get sentence log perplexities\n","    # YOUR CODE HERE\n","    ...\n","\n","cleaned_bigram_lm_perplexity = 2**np.mean(log_perplex_list)\n","\n","print(f\"(cleaned) Bigram language model perplexity is {cleaned_bigram_lm_perplexity}\")"]},{"cell_type":"markdown","id":"e2ea844a","metadata":{"id":"e2ea844a"},"source":["## 1.3 Tri-gram Language Model <a name='trigram_lm'></a>\n","In the Tri-gram language model, we pick/generate next token conditioned only on the two previous tokens. Therefore, for an arbitrary sequence $x_1x_2~...x_n$, its respective probability becomes:\n","$$p(x_1x_2~...x_n) = p(x_1) p(x_2|x_1) ~\\Pi_{i=3} ^n p(x_i|x_{i-2}x_{i-1})$$\n","Let's use the same dataset (`Penn Treebank`) to evaluate this model's perplexity. (We use the dataset that already has the `<stop>` token at the end of each sentence).\n","\n","\n","We estimate $p(x_i|x_{i-1}x_{i-2})$ using the Laplace smoothing with $\\alpha=3 \\cdot 10^{-3}$. First let's write a function that computes these conditional probabilities for the Tri-gram language model."]},{"cell_type":"code","execution_count":null,"id":"abdf0b35","metadata":{"id":"abdf0b35"},"outputs":[],"source":["def get_smoothed_second_order_conditional_probabilities(train_dataset: datasets.arrow_dataset.Dataset,\n","                                                        smoothing_alpha: float):\n","    '''\n","    In this function the conditional probabilities have to be computed based on train_dataset. The output\n","    of the function is a dictionary having keys like (x_{i-2}, x_{i-1}, x_i) as a tuple and the\n","    value being p(x_i | x_{i-2} x_{i-1}).\n","    args:\n","        train_dataset: a Dataset object that can be iterated to get all the sentences\n","        smoothing_alpha: The alpha parameter used in the Laplace smoothing.\n","    output:\n","        word_prob_dict: a dictionary containing the word probabilities\n","        first_order_condition_prob: a dictionary containing the smoothed first order\n","                                    conditional probabilities.\n","        second_order_condition_prob: a dictionary containing the smoothed second order\n","                                     conditional probabilities.\n","    '''\n","    smoothed_second_order_condition_prob = defaultdict(float)  # Note that we shouldn't get zeros for unseen probabilies.\n","\n","    # let's first get the 0th and 1st order conditional probabilities\n","    (word_prob_dict, first_order_condition_prob) = get_smoothed_first_order_conditional_probabilities(\n","        train_dataset, smoothing_alpha)\n","\n","    all_tokens = \" \".join([i[\"sentence\"] for i in train_dataset]).split()\n","    word_frequency_dict = Counter(all_tokens)\n","    vocab_size = len(word_frequency_dict)\n","\n","    # TODO: Get bigram probabilities\n","    all_bigrams = []\n","    for sample in train_dataset:\n","        # YOUR CODE HERE\n","        ...\n","    bigram_frequency_dict = Counter(all_bigrams)\n","\n","\n","    # TODO: Get trigram probabilities\n","    all_trigrams = []\n","    for sample in train_dataset:\n","        # YOUR CODE HERE\n","        ...\n","    trigram_frequency_dict = Counter(all_trigrams)\n","\n","\n","    # TODO: Get smoothed probabilities\n","    # YOUR CODE HERE\n","    smoothed_second_order_condition_prob = ...\n","\n","    return word_prob_dict, first_order_condition_prob, smoothed_second_order_condition_prob\n"]},{"cell_type":"markdown","id":"936a664f","metadata":{"id":"936a664f"},"source":["#### Now let's analyze the Tri-gram language model for different sequences. We first create a function that can output the probability for a given string."]},{"cell_type":"code","execution_count":null,"id":"3f6a692c","metadata":{"id":"3f6a692c"},"outputs":[],"source":["def smoothed_trigram_lm_seq_probability(input_sentence: str,\n","                                        word_prob_dict: dict,\n","                                        word_frequency_dict: dict,\n","                                        bigram_frequency_dict: dict,\n","                                        first_order_condition_prob: dict,\n","                                        second_order_condition_prob: dict,\n","                                        smoothing_alpha: float):\n","    '''\n","    args:\n","        input_sentence: The input sequence string. Here we assume\n","        word_prob_dict: a dictionary containing the word probabilities\n","        word_frequency_dict: a dictionary containing the frequency for every word in vocabulary\n","        bigram_frequency_dict: a dictionary containing the frequency for every bigram in vocabulary\n","        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n","                                    as discussed earlier.\n","        second_order_condition_prob: a dictionary containing the second order conditional probabilities\n","                                     as discussed in the previous function.\n","    output:\n","        probability: The probability of the input_sentence according to the Bi-gram language model\n","    '''\n","\n","    # TODO: compute smoothed trigram sequence probabilities\n","    # YOUR CODE HERE\n","\n","    probability = ...\n","\n","    return probability"]},{"cell_type":"markdown","id":"16865742","metadata":{"id":"16865742"},"source":["#### Now let's compute the probability for sequences in the test dataset, assuming $\\alpha=3\\cdot10^{-3}$ has been used in the Laplace smoothing."]},{"cell_type":"code","execution_count":null,"id":"36feefd8","metadata":{"id":"36feefd8"},"outputs":[],"source":["smoothed_trigram_test_probabilities = []\n","\n","word_frequency_dict = Counter(\" \".join([i[\"sentence\"] for i in ptb_train]).split())\n","\n","all_bigrams = []\n","for sample in ptb_train:\n","    # TODO: get all bigrams\n","    # YOUR CODE HERE\n","    ...\n","bigram_frequency_dict = Counter(all_bigrams)\n","\n","(word_prob_dict,\n"," smoothed_first_order_condition_prob,\n"," smoothed_second_order_condition_prob) = get_smoothed_second_order_conditional_probabilities(ptb_train, 3e-3)\n","\n","# TODO: get smoothed trigram probabilities on ptb_test dataset. Set `smoothing_alpha=3e-3`.\n","# YOUR CODE HERE\n","smoothed_trigram_test_probabilities = ...\n","\n","\n","\n","print(f\"{smoothed_trigram_test_probabilities.count(0)/len(ptb_test)*100}% of samples in the test set have zero probability!\")"]},{"cell_type":"markdown","id":"26ddd040","metadata":{"id":"26ddd040"},"source":["Now we compute the perplexity on the `ptb_test` dataset for the tri-gram language model."]},{"cell_type":"code","execution_count":null,"id":"25ed3d70","metadata":{"id":"25ed3d70"},"outputs":[],"source":["trigram_lm_perplexity = -1\n","\n","# YOUR CODE HERE\n","log_perplex_list = []\n","for idx in range(len(ptb_test)):\n","    ...\n","    # TODO: get trigram sentence probabilities\n","    # YOUR CODE HERE\n","\n","trigram_lm_perplexity = 2**np.mean(log_perplex_list)\n","\n","print(f\"Trigram language model perplexity is {trigram_lm_perplexity}\")"]},{"cell_type":"markdown","id":"e34c59c8","metadata":{"id":"e34c59c8"},"source":["Repeat the same steps but for `cleaned_train_dataset` and `cleaned_test_dataset` datasets where rare tokens (with frequency less than 10) are replaced with `<unk>` token. Do we have a better or a worse perplexity compared to the previous computed perplexity?"]},{"cell_type":"code","execution_count":null,"id":"41b456be","metadata":{"id":"41b456be"},"outputs":[],"source":["cleaned_word_frequency_dict = Counter(\" \".join([i[\"sentence\"] for i in cleaned_train_dataset]).split())\n","\n","all_bigrams = []\n","for sample in cleaned_train_dataset:\n","    # TODO: add all bigrams into `all_bigrams`\n","    # YOUR CODE HERE\n","    ...\n","cleaned_bigram_frequency_dict = Counter(all_bigrams)\n","\n","\n","(cleaned_word_prob_dict,\n"," cleaned_first_order_condition_prob,\n"," cleaned_second_order_condition_prob\n",") = get_smoothed_second_order_conditional_probabilities(cleaned_train_dataset, 3e-3)\n","\n","\n","# TODO: get trigram sequence probabilities on `cleaned_test_dataset` using `smoothed_trigram_lm_seq_probability`\n","cleaned_smoothed_trigram_test_probabilities = ...\n","\n","\n","# compute perplexities\n","log_perplex_list = []\n","for idx in range(len(cleaned_test_dataset)):\n","    sentence_prob = cleaned_smoothed_trigram_test_probabilities[idx]\n","    if sentence_prob==0:\n","        continue\n","    sentence_length = len(cleaned_test_dataset[idx][\"sentence\"].split())\n","    log_perplex_list.append(-np.log2(sentence_prob)/sentence_length)\n","cleaned_Trigram_lm_perplexity = 2**np.mean(log_perplex_list)\n","\n","print(f\"(cleaned) Trigram language model perplexity is {cleaned_Trigram_lm_perplexity}\")"]},{"cell_type":"markdown","id":"2d8e69cc","metadata":{"id":"2d8e69cc"},"source":["#### Discussion\n"," - How are the three discussed models performance compare to each other?\n"," - What is the cost of using N-gram language models for even larger N values?\n"," - What is the effect of vocabulary size on models' perplexities? Can we compare models with different vocabulary sizes?\n"," - What is the perplexity of a language model (vocabulary size of |V|) that given any context (i.e., $x_1 x_2 ... x_{n-1}$) assigns uniform probabilities (for all the tokens in the vocabulary) for the next token?"]},{"cell_type":"markdown","id":"641578b1","metadata":{"id":"641578b1"},"source":["## 2. Task B: Neural Language Models <a name='neural_lm'></a>\n","\n","In this exercise, we will better understand the functioning of some simple neural language models. We first start with a fixed-window neural language model. In the following subsection, we will investigate an RNN-based language model."]},{"cell_type":"markdown","id":"88cee765","metadata":{"id":"88cee765"},"source":["### 2.1 Fixed-Window Neural Language Model <a name='fixed_window_lm'></a>\n","This language model take as input a constant number of tokens, and then outputs a probability distribution for the next token. In this section, we assume the underlying model is a Multi-layer Perceptron (MLP) with a single hidden layer. This model doesn't have the sparsity issue of N-gram language models, but is always limited to a fixed window of tokens.\n","\n","In this section, we don't include the training of the model but rather we use a pretrained model on the same training dataset. We evaluate the language model over the `ptb_test` dataset, to show the power of neural language models, when compared to N-gram language models.\n","\n","More importantly, we use PyTorch modules in this section, so that you get more familiar with its capabilities. Throughout this exercise, we use a `window_size=3` for this model.\n","\n"]},{"cell_type":"markdown","id":"6808da1e","metadata":{"id":"6808da1e"},"source":["Let's first create a dataset of all consecutive tokens of length `window_size` from the `ptb_train` dataset. you can read more about PyTorch datasets and how to create a custom dataset  [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)."]},{"cell_type":"code","execution_count":null,"id":"70f5c3bf","metadata":{"id":"70f5c3bf"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","window_size = 3\n","vocabulary_size = 10000\n","word_emb_dim = 100\n","hidden_dim = 100\n","\n","\n","class FixedWindowDataset(Dataset):\n","    # read more about custom datasets at https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n","    def __init__(self,\n","                 train_dataset: datasets.arrow_dataset.Dataset,\n","                 test_dataset: datasets.arrow_dataset.Dataset,\n","                 window_size: int,\n","                 vocabulary_size: int\n","                ):\n","        self.prepared_train_dataset = self.prepare_fixed_window_lm_dataset(train_dataset, window_size + 1)\n","        self.prepared_test_dataset = self.prepare_fixed_window_lm_dataset(test_dataset, window_size + 1)\n","\n","        dataset_vocab = self.get_dataset_vocabulary(train_dataset)\n","        # defining a dictionary that simply maps tokens to their respective index in the embedding matrix\n","        self.word_to_index = {word: idx for idx,word in enumerate(dataset_vocab)}\n","        self.index_to_word = {idx: word for idx,word in enumerate(dataset_vocab)}\n","\n","        assert vocabulary_size >= len(dataset_vocab) , f\"The dataset vocab size is {len(dataset_vocab)}!\"\n","\n","    def __len__(self):\n","        return len(self.prepared_train_dataset)\n","\n","    def get_encoded_test_samples(self):\n","        all_token_lists = [sample.split() for sample in self.prepared_test_dataset]\n","        all_token_ids = [[self.word_to_index.get(word, self.word_to_index[\"<unk>\"])\n","                          for word in token_list[:-1]]\n","                         for token_list in all_token_lists\n","                        ]\n","        all_next_token_ids = [self.word_to_index.get(token_list[-1], self.word_to_index[\"<unk>\"]) for\n","                              token_list in all_token_lists]\n","        return torch.tensor(all_token_ids), torch.tensor(all_next_token_ids)\n","\n","    def __getitem__(self, idx):\n","        # here we need to transform the data to the format we expect at the model input\n","        token_list = self.prepared_train_dataset[idx].split()\n","        # having a fallback to <unk> token if an unseen word is encoded.\n","        token_ids = [self.word_to_index.get(word, self.word_to_index[\"<unk>\"]) for word in token_list[:-1]]\n","        next_token_id = self.word_to_index.get(token_list[-1], self.word_to_index[\"<unk>\"])\n","        return torch.tensor(token_ids), torch.tensor(next_token_id)\n","\n","    def decode_idx_to_word(self, token_id):\n","        return [self.index_to_word[id_.item()] for id_ in token_id]\n","\n","    def get_dataset_vocabulary(self, train_dataset: datasets.arrow_dataset.Dataset):\n","        vocab = sorted(set(\" \".join([sample[\"sentence\"] for sample in train_dataset]).split()))\n","        # we also add a <start> token to include initial tokens in the sentences in the dataset\n","        vocab += [\"<start>\"]\n","        return vocab\n","\n","    @staticmethod\n","    def prepare_fixed_window_lm_dataset(target_dataset: datasets.arrow_dataset.Dataset,\n","                                        window_size: int):\n","        '''\n","        Please note that for the very first tokens, they will be added like \"<start> <start> Token#1\".\n","        args:\n","            target_dataset: the target dataset where its consecutive tokens of length 'window_size' should be extracted\n","            window_size: the window size for the language model\n","        output:\n","            prepared_dataset: a list of strings each containing 'window_size' tokens.\n","        '''\n","\n","        prepared_dataset = []\n","\n","        # YOUR CODE HERE\n","        # TODO: (1) filter out sequences with length less than `window_size`\n","        #       (2) prepend \"<start>\" tokens at the beginning of sequences\n","        #       (3) add all sub-sequence with length as `window_size` into prepared_dataset\n","\n","        return prepared_dataset\n","\n"]},{"cell_type":"code","execution_count":null,"id":"524f4a18","metadata":{"id":"524f4a18"},"outputs":[],"source":["fixed_window_dataset = FixedWindowDataset(ptb_train, ptb_test, window_size, vocabulary_size)\n","\n","# let's create a simple dataloader for this dataset\n","train_dataloader =  DataLoader(fixed_window_dataset, batch_size=8, shuffle=True)"]},{"cell_type":"markdown","id":"5b44e90d","metadata":{"id":"5b44e90d"},"source":["Now, let's define the underlying PyTorch model for the language model. You can read more about PyTorch models [here](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html).\n","\n","**Note**: Here in the forward pass, we compute the negative log-likelihood after passing through the MLP layers. Here we use `torch.nn.LogSoftmax`, as it's numerically more stable than doing seperately `softmax` followed by taking its logarithm."]},{"cell_type":"code","execution_count":null,"id":"e3ea691f","metadata":{"id":"e3ea691f"},"outputs":[],"source":["import torch.optim as optim\n","\n","class Fixed_window_language_model(torch.nn.Module):\n","    def __init__(self, emb_dim, hidden_dim, window_size, vocab_size=10000):\n","        super().__init__()\n","\n","        self.window_size = window_size\n","        self.emb_dim = emb_dim\n","        self.word_embeddings = torch.nn.Embedding(vocab_size, emb_dim) # word embeddings\n","        self.linear1 = torch.nn.Linear(window_size * emb_dim, hidden_dim) # first linear layer\n","        self.activation_func = torch.tanh # the activation function\n","        self.linear2 = torch.nn.Linear(hidden_dim, vocab_size) # second linear layer\n","\n","        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n","        self.criterion = torch.nn.NLLLoss()\n","\n","    def forward(self, input_ids, labels):\n","        inputs_embeds = self.word_embeddings(input_ids)\n","        concat_input_embed = inputs_embeds.reshape(-1, self.emb_dim * self.window_size)\n","        hidden_state = self.activation_func( self.linear1(concat_input_embed) )\n","        logits = self.log_softmax( self.linear2(hidden_state) )\n","        loss = self.criterion(logits, labels)\n","\n","        return loss\n"]},{"cell_type":"markdown","id":"4dae5397","metadata":{"id":"4dae5397"},"source":["Now let's see how easy it is to train a model with PyTorch! (we provide a trained model in the cell after train, so that you can just start using the model without going through the time-consuming training)"]},{"cell_type":"code","execution_count":null,"id":"93779901","metadata":{"id":"93779901"},"outputs":[],"source":["# defining the model\n","model_fixed_window = Fixed_window_language_model(emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n","                                                 window_size=window_size, vocab_size=vocabulary_size)\n","\n","# defining the optimizer\n","optimizer = optim.SGD(model_fixed_window.parameters(),\n","                      lr=0.005,\n","                      momentum=0.9)"]},{"cell_type":"code","execution_count":null,"id":"77626d29","metadata":{"scrolled":true,"id":"77626d29"},"outputs":[],"source":["for epoch in range(2):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(train_dataloader):\n","        # get the inputs; data is a tuple of (context, target)\n","        context, target = data\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        loss = model_fixed_window(context, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 5000 == 4999. :    # print every 5000 mini-batches\n","            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 5000:.3f}')\n","            running_loss = 0.0\n","\n","print('Finished Training')\n","\n","# saving the trained model\n","torch.save(model_fixed_window.state_dict(), \"fixed_window_model.pt\")"]},{"cell_type":"markdown","id":"0483dd01","metadata":{"id":"0483dd01"},"source":["We provide a trained model, so that you can start using it right away"]},{"cell_type":"code","execution_count":null,"id":"54ba51b1","metadata":{"id":"54ba51b1"},"outputs":[],"source":["fixed_window_checkpoint_file = \"fixed_window_model.pt\"\n","model_fixed_window.load_state_dict(torch.load(fixed_window_checkpoint_file))"]},{"cell_type":"code","execution_count":null,"id":"b806f375","metadata":{"id":"b806f375"},"outputs":[],"source":["# context and 'target' ids (target is the next word after the context)\n","test_token_ids, test_target_ids = fixed_window_dataset.get_encoded_test_samples()"]},{"cell_type":"markdown","id":"f87e283d","metadata":{"id":"f87e283d"},"source":["We now have the `test_token_ids`, `test_target_ids` tensors for the test dataset. The `test_token_ids` are the context ids and `test_target_ids` are the respective **next token** (a.k.a. target here) for these contexts.\n","#### Using the trained model, implement a function that can output the loss for the discussed test dataset. How can we generally decide if the model is overfitted to the train dataset or not?"]},{"cell_type":"code","execution_count":null,"id":"e352e019","metadata":{"id":"e352e019"},"outputs":[],"source":["def generate_test_dataset_loss(model: torch.nn.Module,\n","                               test_token_ids: torch.Tensor,\n","                               test_target_ids: torch.Tensor):\n","    '''\n","    args:\n","        model: fixed-window language model\n","        test_token_ids: the context ids in a single tensor.\n","        test_target_ids: the target ids (next token after the context) in a single tensor.\n","    output:\n","        avg_test_loss: The average loss of model over test dataset.\n","    '''\n","    batch_size = 4\n","    test_loss = []\n","\n","\n","    # YOUR CODE HERE\n","\n","    return avg_test_loss\n","\n","\n","test_dataset_loss = generate_test_dataset_loss(model_fixed_window, test_token_ids, test_target_ids)\n","print(f\"Test dataset loss is {test_dataset_loss}\")"]},{"cell_type":"markdown","id":"085d7476","metadata":{"id":"085d7476"},"source":["#### Using the trained fixed-window model, implement a function that can output entropy for a given sequence."]},{"cell_type":"code","execution_count":null,"id":"db54e454","metadata":{"id":"db54e454"},"outputs":[],"source":["def get_seqeuence_entropy_fixed_window_lm(model: torch.nn.Module,\n","                                              input_sequence: str,\n","                                              window_size: int,\n","                                              word_to_idx: dict):\n","    '''\n","    Note that e.g., in order to get the first token probability, you need to pass a sequence\n","    like \"<start> <start> <start>\" (prefix padding) to the neural model. In a similar fashion, we need to pass\n","    \"<start> <start> TOKEN#1\" for getting the probability of the second token.\n","    args:\n","        model: fixed-window language model\n","        input_sequence: the sequence for which we want to calculate the probability\n","        window_size: the size of window for the language model\n","        word_to_idx: a mapping from words to the embedding indices (to encode tokens before being\n","                     passed to model). You can get this dict from 'fixed_window_dataset.word_to_index'\n","    output:\n","        sequence_entropy: the entropy for the input sequence using the trained model\n","    '''\n","    modified_sentence = \"<start> \" * (window_size) + input_sequence\n","\n","    token_list = modified_sentence.split()\n","    encoded_context = []\n","    encoded_target = []\n","\n","    for idx in range(len(token_list)-window_size):\n","        # TODO: add items into encoded_context and encoded_target.\n","        # YOUR CODE HERE\n","        encoded_context.append(...)\n","        encoded_target.append(...)\n","\n","    encoded_context = torch.tensor(encoded_context)\n","    encoded_target = torch.tensor(encoded_target)\n","\n","    # TODO: passing the context (and respective labels) to the fixed-window LM to get average NLL.\n","    # YOUR CODE HERE\n","    with torch.no_grad():\n","        sequence_entropy = ...\n","\n","    return sequence_entropy"]},{"cell_type":"markdown","id":"7d055835","metadata":{"id":"7d055835"},"source":["#### Compute the perplexity for the trained fixed-window language model over `ptb_test` dataset using the previous function. How does it perform compared to N-gram language models we discussed earlier?"]},{"cell_type":"code","execution_count":null,"id":"f4289886","metadata":{"id":"f4289886"},"outputs":[],"source":["perplexity = -1\n","\n","# YOUR CODE HERE\n","\n","print(f\"The fixed-window model perplexity over test dataset is {perplexity}\")"]},{"cell_type":"markdown","id":"240c3d89","metadata":{"id":"240c3d89"},"source":["### 2.2 RNN-based Language Model <a name='rnn_lm'></a>\n","To address the need for a neural architecture that can proceed with any length input (as opposed to the fixed-window model that can only process a fixed number of tokens), we implement the Recurrent Neural Network (RNN). The core idea behind is that we can apply the same weight W repeatedly.\n","\n","An advatange of RNN model compared to fixed-window langauage model is that we can pass a given sentence at once, instead of passing it in many windows of size `window_size`. Moreover, the language model has the ability to look behind further that a fixed number of tokens.\n","\n"," As we already did a neural model training exercise for the previous neural model, we only provide a trained LM at this section, so that you can focus only on the analysis part.\n","\n","You can find the dataset structure as well as the RNN architecture in the `rnn_utils.py` file."]},{"cell_type":"code","execution_count":null,"id":"fc631bc1","metadata":{"id":"fc631bc1"},"outputs":[],"source":["from rnn_utils import RNNDataset, RNN_language_model\n","\n","vocabulary_size = 10000\n","word_emb_dim = 200\n","hidden_dim = 200\n","\n","rnn_dataset = RNNDataset(ptb_train, ptb_test, vocabulary_size)\n","\n","# if gpu is available, we puts the model on it\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Here we need a <pad> token for the RNN model, in order to have a batch of sequences with difference sizes\n","pad_idx = rnn_dataset.pad_idx # the index for <pad> token\n","rnn_model = RNN_language_model(vocab_size=vocabulary_size, emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n","                               pad_idx=pad_idx)\n","rnn_model.to(device)"]},{"cell_type":"markdown","id":"25137a9f","metadata":{"id":"25137a9f"},"source":["load the model weights using the state_dict in `rnn_model.pt` file."]},{"cell_type":"code","execution_count":null,"id":"52adb1f9","metadata":{"id":"52adb1f9"},"outputs":[],"source":["# YOUR CODE HERE\n","# TODO: Load the model state_dict and map to the correct device"]},{"cell_type":"markdown","id":"f06eb967","metadata":{"id":"f06eb967"},"source":["As the training of an RNN model is time-consuming, we provide a trained language model on this dataset (`rnn_model.pt`), so that you can just analyze the model performance here.\n","As mentioned above, as RNN can get sequences with varying lengths, the input sequences should be padded with a special token like `<pad>`, so that we can create a batch of sentences. The output of the defined RNN model (see the architecture detail `rnn_utils.py`) is the model's entropy over the input data.\n","\n","#### First get the encoded test samples of `ptb_test` dataset, and then pass these (already padded) sentences to the RNN model to get the respective entropy values. Compute the perplexity of the model and compare it with previous approaches.\n","**HINT**: Read `get_encoded_test_samples` method of `rnn_dataset` and all model attribution and methods from `RNN_language_model`, e.g. how to perform forward pass and loss computations.\n"]},{"cell_type":"code","execution_count":null,"id":"09d7b99f","metadata":{"id":"09d7b99f"},"outputs":[],"source":["test_perplexity = -1\n","\n","test_token_ids = rnn_dataset.get_encoded_test_samples()\n","\n","test_loss = []\n","eval_batch_size = 8\n","\n","with torch.no_grad():\n","    for data_idx in range(0, test_token_ids.shape[0], eval_batch_size):\n","        # TODO: get one test batch with batch_size = eval_batch_size\n","        context_batch = ...\n","\n","        batch_max_seq_length = (context_batch!=rnn_dataset.pad_idx).sum(dim=1).max().item()\n","        context_batch = context_batch[:, :batch_max_seq_length]\n","\n","        # TODO: get loss on one batch with rnn_model\n","        batch_loss = ...\n","        test_loss.append(batch_loss.item())\n","avg_test_loss = np.array(test_loss).mean()\n","test_perplexity = np.exp(avg_test_loss)\n","\n","print(f\"The model perplexity is {test_perplexity}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"vscode":{"interpreter":{"hash":"a8ced340a52f9326f5856e1d63a73f97bd9f0a225610b549ff7b502d766a19ce"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}