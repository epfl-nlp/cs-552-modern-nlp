{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV3YaGFE03v1"
   },
   "source": [
    "# ðŸ“š Exercise Session - Week 3: Attention + Transformers\n",
    "**Main Topics**: Attention & Transformers for Sequence-to-Sequence Modeling\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This weekâ€™s session dives into Transformers for sequence-to-sequence (Seq2Seq) tasks, with a special focus on machine translation and attention visualization. By the end, you will have built and trained a Transformer model capable of translating text from one language to anotherâ€”and youâ€™ll see how attention helps it learn alignments across tokens.\n",
    "\n",
    "1. [**TASK A:** Transformer Implementation](#Task-A:-Transformer-Implementation)\n",
    "- Build a BPE tokenizer\n",
    "- Implement a Transformer encoder-decoder model using PyTorch\n",
    "\n",
    "2. [**TASK B:** Train a Machine Translation Model](#Task-B:-Train-a-Machine-Translation-Model)\n",
    "- Use Transformer from Task A to train a machine translation model\n",
    "- Visualize the cross-attention weights\n",
    "\n",
    "3. **Optional Extensions** \n",
    "- Learning Rate Scheduler: Try adding a scheduler (like Warmup or ReduceLROnPlateau) to potentially improve convergence.\n",
    "- Periodic Validation: Every `m` batches or at epochâ€™s end, evaluate on a validation set to track loss or metrics like BLEU.\n",
    "- Use PyTorch `DataLoader`\n",
    "\n",
    "**Tips & Hints**\n",
    "- **Overfitting on a Single Example**: If your model cannot easily learn one sample to near-perfect accuracy, it often indicates an implementation bug or mismatch in shapes/masks.\n",
    "- **Masking**: Pay careful attention to causal masks in the decoder, ensuring the model does not see future tokens.\n",
    "- **Debugging**: Print shapes and partial outputs, or watch the attention scores to confirm they behave as expected.\n",
    "\n",
    "\n",
    "> **By the end of the session you will be able to:**\n",
    "> - âœ…  Implement an encoder-decoder Transformer model using PyTorch\n",
    "> - âœ…  Train your model on a machine translation corpus\n",
    "> - âœ…  Understand attention mechanism within the Transformer architecture\n",
    "> - âœ…  Be more interested in NLP ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yDoUnLJKoIK"
   },
   "source": [
    "## Task A: Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Reminder**\n",
    "We will be implementing the following encoder-decoder transformer architecture following the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper!\n",
    "\n",
    "![Transformer Architecture](transformer_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kH-A3gdHF7o"
   },
   "source": [
    "In this part, you will implement an encoder-decoder Transformer model using [Pytorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T14:42:37.478843Z",
     "iopub.status.busy": "2024-02-22T14:42:37.478185Z",
     "iopub.status.idle": "2024-02-22T14:42:50.715331Z",
     "shell.execute_reply": "2024-02-22T14:42:50.714317Z",
     "shell.execute_reply.started": "2024-02-22T14:42:37.478777Z"
    },
    "id": "vFN2GC6PX1_P",
    "outputId": "aa984dba-d338-4f62-eaba-1c8006dc623e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # These layers transform the input embeddings to queries, keys, and values\n",
    "        self.query_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_proj   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Final projection after concatenating heads\n",
    "        self.out_proj   = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query, key, value: [batch_size, seq_len, embed_dim]\n",
    "        mask: [batch_size, 1, seq_len, seq_len] or None (optional)\n",
    "\n",
    "        returns:\n",
    "          - output: [batch_size, seq_len, embed_dim]\n",
    "          - attn_weights: [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1) Linear projections\n",
    "        # TODO: project query, key, and value via the layers: self.query_proj, self.key_proj, self.value_proj\n",
    "        # shape after projection: [batch_size, seq_len, embed_dim]\n",
    "        Q = ...\n",
    "        K = ...\n",
    "        V = ...\n",
    "\n",
    "        # 2) Split into multiple heads\n",
    "        # We want shape: [batch_size, num_heads, seq_len, head_dim]\n",
    "        # TODO: reshape the Q, K, V so that we chunk embed_dim into (num_heads, head_dim)\n",
    "        Q = ...\n",
    "        K = ...\n",
    "        V = ...\n",
    "\n",
    "        # 3) Scaled dot-product attention\n",
    "        #    attention_scores = Q x K^T / sqrt(head_dim)\n",
    "        #    then apply optional mask (if not None)\n",
    "        #    then softmax, then dropout, then multiply by V\n",
    "\n",
    "        # TODO: compute attention_scores\n",
    "        # attention_scores shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "        ...\n",
    "        \n",
    "        # TODO: apply mask if given\n",
    "        # if mask is not None:\n",
    "        #     attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # TODO: normalize attention_scores\n",
    "        ...\n",
    "        \n",
    "        # TODO: compute attention_output\n",
    "        # shape: [batch_size, num_heads, seq_len, head_dim]\n",
    "        ...\n",
    "\n",
    "        # 4) Concat heads\n",
    "        # We want shape: [batch_size, seq_len, embed_dim]\n",
    "        # (i.e. combine num_heads and head_dim back into embed_dim)\n",
    "        ...\n",
    "        \n",
    "        # 5) Final linear projection\n",
    "        ...\n",
    "        \n",
    "        return output, attn_weights  # (attn_weights = normalized_weights for visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.linear1   = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2   = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.norm1     = nn.LayerNorm(embed_dim)\n",
    "        self.norm2     = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1  = nn.Dropout(dropout)\n",
    "        self.dropout2  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # 1) Multi-head self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=mask)\n",
    "        x = x + attn_output  # residual\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # 2) Feed-forward\n",
    "        ff_output = self.linear2(F.relu(self.linear1(x)))\n",
    "        x = x + ff_output  # residual\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # Multi-Head Attention Layers\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)   # Students already have MHA\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "\n",
    "        # Feed-Forward Layers\n",
    "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "        # LayerNorms\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, tgt_seq_len, embed_dim] -- decoder input embeddings\n",
    "            enc_output: [batch_size, src_seq_len, embed_dim] -- encoder output\n",
    "            tgt_mask: mask for target self-attention (e.g., causal + padding)\n",
    "            cross_attn_mask: mask for encoder-decoder attention (padding mask for source)\n",
    "        Returns:\n",
    "            x: [batch_size, tgt_seq_len, embed_dim] -- updated decoder features\n",
    "            attn_weights: attention weights from cross-attention (for visualization, etc.)\n",
    "        \"\"\"\n",
    "\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=tgt_mask)\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # 1) TODO: Implement cross-attention with encoder output\n",
    "        #    - Query = x (decoder), Key/Value = enc_output\n",
    "        #    - residual connection + self.norm2\n",
    "\n",
    "        # 2) TODO: Implement feed-forward sub-layer\n",
    "        #    - pass x through self.linear1, then an activation (e.g., F.relu)\n",
    "        #    - then self.linear2\n",
    "        #    - Add residual + self.norm3\n",
    "\n",
    "        # TODO: return final x and the cross-attention weights\n",
    "        return x, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len=1024, dropout=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = nn.Embedding(max_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        \"\"\"\n",
    "        src: [batch_size, src_seq_len]\n",
    "        src_mask: (optional)\n",
    "        \"\"\"\n",
    "        x = self.embedding(src) * math.sqrt(self.embed_dim)\n",
    "\n",
    "        # TODO: Add positional encoding to the input embeddings\n",
    "        ...\n",
    "\n",
    "        # TODO: Implement the forward pass through the encoder layers\n",
    "        ...\n",
    "        \n",
    "        return x  # shape: [batch_size, src_seq_len, embed_dim]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len=1024, dropout=0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = nn.Embedding(max_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, enc_output, tgt_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        tgt: [batch_size, tgt_seq_len]\n",
    "        enc_output: [batch_size, src_seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.embedding(tgt) * math.sqrt(self.embed_dim)\n",
    "        # TODO: Add positional encoding to the input embeddings\n",
    "        ...\n",
    "\n",
    "        attn_weights = None\n",
    "        for layer in self.layers:\n",
    "            x, attn_weights = layer(x, enc_output, tgt_mask=tgt_mask, cross_attn_mask=cross_attn_mask)\n",
    "\n",
    "        # Final projection to vocabulary\n",
    "        logits = self.out_proj(x)  # [batch_size, tgt_seq_len, vocab_size]\n",
    "        return logits, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(seq_len, device=None):\n",
    "    \"\"\"\n",
    "    Returns a 2D causal mask of shape [seq_len, seq_len], \n",
    "    where True means 'allowed to attend' and False means 'disallowed'.\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=device))\n",
    "\n",
    "def expand_causal_mask(causal_2d, batch_size, num_heads):\n",
    "    # causal_2d: shape [seq_len, seq_len]\n",
    "    # expand to [batch_size, num_heads, seq_len, seq_len]\n",
    "    causal_4d = causal_2d.unsqueeze(0).unsqueeze(0)  # => [1,1,seq_len,seq_len]\n",
    "    causal_4d = causal_4d.expand(batch_size, num_heads, causal_2d.size(0), causal_2d.size(1))\n",
    "    return causal_4d\n",
    "\n",
    "def expand_padding_mask(pad_mask_2d, num_heads):\n",
    "    # pad_mask_2d: shape [batch_size, seq_len], 1 = valid, 0 = pad\n",
    "    # step 1) Convert to bool if needed\n",
    "    pad_mask_bool = pad_mask_2d.bool()  # shape [batch_size, seq_len]\n",
    "    # step 2) unsqueeze => [batch_size, 1, 1, seq_len]\n",
    "    pad_mask_4d = pad_mask_bool.unsqueeze(1).unsqueeze(2)\n",
    "    # step 3) broadcast across the query dimension\n",
    "    batch_size, _, _, seq_len = pad_mask_4d.shape\n",
    "    pad_mask_4d = pad_mask_4d.expand(batch_size, num_heads, seq_len, seq_len)\n",
    "    return pad_mask_4d\n",
    "\n",
    "\n",
    "def build_decoder_mask(\n",
    "    pad_mask_2d: torch.Tensor, \n",
    "    num_heads: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine the target padding mask with the causal mask \n",
    "    to produce a final decoder mask of shape:\n",
    "    [batch_size, num_heads, seq_len, seq_len], \n",
    "    where True means 'allowed', False means 'masked out'.\n",
    "    \"\"\"\n",
    "    device = pad_mask_2d.device\n",
    "    batch_size, seq_len = pad_mask_2d.shape\n",
    "\n",
    "    # 1) Build the 2D causal mask\n",
    "    causal_2d = build_causal_mask(seq_len, device=device)\n",
    "\n",
    "    # 2) Expand to 4D\n",
    "    causal_4d = expand_causal_mask(causal_2d, batch_size, num_heads)\n",
    "\n",
    "    # 3) Expand the padding mask to 4D\n",
    "    pad_4d = expand_padding_mask(pad_mask_2d, num_heads)\n",
    "\n",
    "    # 4) Final mask = causal AND pad\n",
    "    final_mask = causal_4d & pad_4d  # shape [batch_size, num_heads, seq_len, seq_len]\n",
    "    return final_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim=512, num_heads=8, hidden_dim=2048, \n",
    "                 num_layers=6, dropout=0):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.encoder = Encoder(src_vocab_size, embed_dim, num_heads, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, embed_dim, num_heads, hidden_dim, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, cross_attn_mask=None):\n",
    "\n",
    "        if src_mask is not None:\n",
    "            src_mask = src_mask[:, None, None, :]\n",
    "            src_mask = src_mask.expand(-1, -1, src_mask.size(-1), -1)\n",
    "\n",
    "        tgt_mask = build_decoder_mask(tgt_mask, num_heads=self.num_heads)\n",
    "\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        logits, attn_weights = self.decoder(tgt, enc_output, tgt_mask, cross_attn_mask)\n",
    "        return logits, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B: Train a Machine Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to download the French-English translation dataset using [this link](https://drive.google.com/file/d/1cPKNjpU7PiqA33GzV0yICwDjZ_0ysKjO/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_from_disk(\"wmt14_fr_en\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a BPE Tokenizer\n",
    "\n",
    "> You will learn about BPE tokenization later in the semester. Here we provide you with the code necessary to train your own tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "def batch_iterator(dataset, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Yields batches of text from the Hugging Face dataset.\n",
    "    Args:\n",
    "        dataset: a huggingface dataset split (e.g. train_dataset)\n",
    "        batch_size: how many samples per batch\n",
    "        text_column: name of the column containing the text\n",
    "    \"\"\"\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        # Extract a batch of examples\n",
    "        batch = dataset[i : i + batch_size]\n",
    "\n",
    "        # 'batch' is now a list of strings (if text_column is indeed text).\n",
    "        yield batch\n",
    "\n",
    "\n",
    "fr_training_set = [dataset[\"train\"][i]['translation']['fr'] for i in range(100_000)]\n",
    "en_training_set = [dataset[\"train\"][i]['translation']['en'] for i in range(100_000)]\n",
    "training_set = en_training_set + fr_training_set\n",
    "\n",
    "# 1) Initialize tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# 2) Train from the iterator\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=batch_iterator(training_set, batch_size=1000),\n",
    "    vocab_size=32_000,  # Choose your vocab size\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]  # Or any set of special tokens you prefer\n",
    ")\n",
    "\n",
    "# 3) Save the tokenizer\n",
    "tokenizer.save(\"my_bytelevel_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 439, 8722, 6667, 1727,  291, 3255]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "['The', 'Ä quick', 'Ä bro', 'wn', 'Ä f', 'ox']\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"my_bytelevel_tokenizer.json\",  # or the two files from the BPE approach\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\"\n",
    ")\n",
    "\n",
    "encoded_input = hf_tokenizer(\"The quick brown fox\", return_tensors=\"pt\", add_special_tokens=True)\n",
    "decoded_output = hf_tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n",
    "\n",
    "print(encoded_input)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "max_len = 256\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Model parameters\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "hidden_dim = 2048\n",
    "num_layers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing the Transformer model...\")\n",
    "model = Transformer(\n",
    "    src_vocab_size=hf_tokenizer.vocab_size, \n",
    "    tgt_vocab_size=hf_tokenizer.vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Print the number of parameters\n",
    "print(f\"> Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Compute total number of batches in the training set\n",
    "num_batches = int(np.ceil(len(dataset['train']) / batch_size))\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=hf_tokenizer.pad_token_id)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# Switch model to training mode\n",
    "model.train()\n",
    "for epoch_num in range(epochs):\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        train_batch = dataset['train'][batch_idx * batch_size : (batch_idx + 1) * batch_size]['translation']\n",
    "\n",
    "        src_train = [example['fr'] for example in train_batch]\n",
    "        tgt_train = [example['en'] for example in train_batch]\n",
    "        \n",
    "        src_tokens = hf_tokenizer(\n",
    "            src_train, \n",
    "            return_tensors='pt', \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=max_len\n",
    "        ).to(device)\n",
    "\n",
    "        tgt_tokens = hf_tokenizer(\n",
    "            tgt_train, \n",
    "            return_tensors='pt', \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=max_len\n",
    "        ).to(device)\n",
    "\n",
    "        # 1) TODO: Prepend BOS token to target tokens\n",
    " \n",
    "        # 2) TODO: Forward pass through the Transformer\n",
    "        logits, _ = ...\n",
    "\n",
    "        # 3) TODO: Shift labels to the right (teacher forcing)\n",
    "        \n",
    "        # 4) TODO: Compute loss\n",
    "        loss = ...\n",
    "\n",
    "        # 5) TODO: Backpropagate and update weights\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch_num}, Batch {batch_idx}/{num_batches}, Loss: {loss.item():.4f}\", end='\\r')\n",
    "\n",
    "    print(f\"Epoch {epoch_num} Finished! Avg Loss: {np.mean(train_losses):.4f}\")\n",
    "\n",
    "\n",
    "print(\"Saving model checkpoint...\")\n",
    "torch.save(model.state_dict(), \"transformer_model.pt\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.lineplot(x=range(len(train_losses)), y=train_losses, marker='o', markersize=5)\n",
    "plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Some Examples\n",
    "\n",
    "Generate using greedy decoding. You will learn more about it in class later in the semester.\n",
    "\n",
    "Here is a [link for a checkpoint](https://drive.google.com/file/d/130dDwMBJGhvSEFQdHkxaU-R5IjlSbO2j/view?usp=sharing) that you can use for prediction and attention weight visualization.\n",
    "It was trained using the following configuration\n",
    "```\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "hidden_dim = 2048\n",
    "num_layers = 4\n",
    "vocab_size = 32000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = Transformer(\n",
    "    src_vocab_size=hf_tokenizer.vocab_size, \n",
    "    tgt_vocab_size=hf_tokenizer.vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "state_dict = torch.load(\"transformer_model.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "src_val = [dataset['train'][i]['translation']['fr'] for i in range(5)]\n",
    "tgt_val = [dataset['train'][i]['translation']['en'] for i in range(5)]\n",
    "\n",
    "src_tokens_val = hf_tokenizer(src_val, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        # Prepare inputs\n",
    "        src = src_tokens_val['input_ids'][i].unsqueeze(0).to(device)\n",
    "        tgt = torch.tensor([[hf_tokenizer.bos_token_id]], dtype=torch.long, device=device)\n",
    "\n",
    "        # Greedy decode up to max_len\n",
    "        for step in range(max_len):\n",
    "            # Create tgt padding mask\n",
    "            tgt_mask = torch.ones_like(tgt).bool().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, _ = model(src, tgt, tgt_mask=tgt_mask)\n",
    "\n",
    "            # Get the most probable token at the current step\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1)\n",
    "\n",
    "            # Append\n",
    "            tgt = torch.cat([tgt, next_token.unsqueeze(-1)], dim=1)\n",
    "\n",
    "            # End conditions\n",
    "            if next_token.item() in [hf_tokenizer.eos_token_id, hf_tokenizer.pad_token_id]:\n",
    "                break\n",
    "\n",
    "        # Decode\n",
    "        translation = hf_tokenizer.decode(tgt[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Source:      {src_val[i]}\")\n",
    "        print(f\"  Translation: {translation}\")\n",
    "        print(f\"  Target:      {tgt_val[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLXazZQx5roi"
   },
   "source": [
    "## Analyze Cross-Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "900FaGrbMCms"
   },
   "source": [
    "- Choose one sample sentence, output attention weights for each token using heatmap\n",
    "- Which pairs of the token have the greatest attention weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T15:18:26.595566Z",
     "iopub.status.busy": "2024-02-22T15:18:26.594626Z",
     "iopub.status.idle": "2024-02-22T15:18:27.145253Z",
     "shell.execute_reply": "2024-02-22T15:18:27.144212Z",
     "shell.execute_reply.started": "2024-02-22T15:18:26.595530Z"
    },
    "id": "QQMMFUipeVkd",
    "outputId": "f5822cd3-9fdf-4af5-9333-1eef3fad1c4e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_cross_attention(\n",
    "    attn_weights, \n",
    "    source_tokens, \n",
    "    target_tokens, \n",
    "    batch_idx=0, \n",
    "    head_idx=0, \n",
    "    title=\"Cross-Attention\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize cross-attention weights for a given batch and head.\n",
    "\n",
    "    Args:\n",
    "        attn_weights: Tensor of shape [batch_size, num_heads, tgt_len, src_len]\n",
    "            Cross-attention weights from your Transformer decoder, \n",
    "            typically returned alongside logits in a (logits, attn_weights) tuple.\n",
    "        source_tokens: List of source tokens (strings) for the batch_idx sample.\n",
    "        target_tokens: List of target tokens (strings) for the batch_idx sample.\n",
    "        batch_idx: Which batch element to visualize (default=0).\n",
    "        head_idx: Which attention head to visualize (default=0).\n",
    "        title: Title for the plot.\n",
    "\n",
    "    Example Usage:\n",
    "        # Suppose attn_weights has shape [batch_size, num_heads, tgt_len, src_len]\n",
    "        # and you have the corresponding token lists for the source and target:\n",
    "        visualize_cross_attention(attn_weights, src_tokens, tgt_tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Extract the attention for the specified batch & head\n",
    "    #    shape: [tgt_len, src_len]\n",
    "    attn = attn_weights[batch_idx, head_idx].detach().cpu().numpy()\n",
    "\n",
    "    tgt_len, src_len = attn.shape\n",
    "\n",
    "    # 2) Plot the heatmap\n",
    "    plt.figure(figsize=(min(12, 1 + 0.5 * src_len), min(6, 1 + 0.5 * tgt_len)))\n",
    "    sns.heatmap(attn, \n",
    "                vmin=0.0, vmax=1.0, \n",
    "                cmap=\"Blues\", \n",
    "                xticklabels=source_tokens, \n",
    "                yticklabels=target_tokens, \n",
    "                cbar=True)\n",
    "\n",
    "    plt.title(f\"{title} (batch={batch_idx}, head={head_idx})\")\n",
    "    plt.xlabel(\"Source Tokens\")\n",
    "    plt.ylabel(\"Target Tokens\")\n",
    "\n",
    "    # Rotate the x-axis labels if tokens are long\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T15:18:40.105997Z",
     "iopub.status.busy": "2024-02-22T15:18:40.105643Z",
     "iopub.status.idle": "2024-02-22T15:18:40.181096Z",
     "shell.execute_reply": "2024-02-22T15:18:40.180179Z",
     "shell.execute_reply.started": "2024-02-22T15:18:40.105972Z"
    },
    "id": "HOxwg2xw52rG",
    "outputId": "f4b013bd-85e3-44ba-eba4-6656db8963cf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "src_val = \"Le renard brun rapide saute par-dessus le chien paresseux.\"\n",
    "tgt_val = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the source and target\n",
    "src_tokens_val = hf_tokenizer(src_val, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "tgt_tokens_val = hf_tokenizer(tgt_val, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "\n",
    "print(src_tokens_val[\"input_ids\"].shape)\n",
    "print(tgt_tokens_val[\"input_ids\"].shape)\n",
    "\n",
    "# Run the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits, attn_weights = model(\n",
    "        src=src_tokens_val['input_ids'],\n",
    "        tgt=tgt_tokens_val['input_ids'],\n",
    "        src_mask=src_tokens_val['attention_mask'],\n",
    "        tgt_mask=tgt_tokens_val['attention_mask']\n",
    "    )\n",
    "\n",
    "# Decode the source and target sequences\n",
    "src_tokens = hf_tokenizer.convert_ids_to_tokens(src_tokens_val[\"input_ids\"][0])\n",
    "tgt_tokens = hf_tokenizer.convert_ids_to_tokens(tgt_tokens_val[\"input_ids\"][0])\n",
    "\n",
    "# Visualize the attention weights\n",
    "for i in range(num_heads):\n",
    "    visualize_cross_attention(\n",
    "        attn_weights=attn_weights,\n",
    "        source_tokens=src_tokens,\n",
    "        target_tokens=tgt_tokens,\n",
    "        batch_idx=0,\n",
    "        head_idx=i\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpXzr36fLUtI"
   },
   "source": [
    "## Congrats! You can now train a simple machine translator by your own ;)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4480106,
     "sourceId": 7679312,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8ced340a52f9326f5856e1d63a73f97bd9f0a225610b549ff7b502d766a19ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
