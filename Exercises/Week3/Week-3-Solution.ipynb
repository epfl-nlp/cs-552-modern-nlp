{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV3YaGFE03v1"
   },
   "source": [
    "# ðŸ“š Exercise Session - Week 3: Attention + Transformers\n",
    "**Main Topics**: Attention & Transformers for Sequence-to-Sequence Modeling\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This weekâ€™s session dives into Transformers for sequence-to-sequence (Seq2Seq) tasks, with a special focus on machine translation and attention visualization. By the end, you will have built and trained a Transformer model capable of translating text from one language to anotherâ€”and youâ€™ll see how attention helps it learn alignments across tokens.\n",
    "\n",
    "1. [**TASK A:** Transformer Implementation](#Task-A:-Transformer-Implementation)\n",
    "- Build a BPE tokenizer\n",
    "- Implement a Transformer encoder-decoder model using PyTorch\n",
    "\n",
    "2. [**TASK B:** Train a Machine Translation Model](#Task-B:-Train-a-Machine-Translation-Model)\n",
    "- Use Transformer from Task A to train a machine translation model\n",
    "- Visualize the cross-attention weights\n",
    "\n",
    "3. **Optional Extensions** \n",
    "- Learning Rate Scheduler: Try adding a scheduler (like Warmup or ReduceLROnPlateau) to potentially improve convergence.\n",
    "- Periodic Validation: Every `m` batches or at epochâ€™s end, evaluate on a validation set to track loss or metrics like BLEU.\n",
    "- Use PyTorch `DataLoader`\n",
    "\n",
    "**Tips & Hints**\n",
    "- **Overfitting on a Single Example**: If your model cannot easily learn one sample to near-perfect accuracy, it often indicates an implementation bug or mismatch in shapes/masks.\n",
    "- **Masking**: Pay careful attention to causal masks in the decoder, ensuring the model does not see future tokens.\n",
    "- **Debugging**: Print shapes and partial outputs, or watch the attention scores to confirm they behave as expected.\n",
    "\n",
    "\n",
    "> **By the end of the session you will be able to:**\n",
    "> - âœ…  Implement an encoder-decoder Transformer model using PyTorch\n",
    "> - âœ…  Train your model on a machine translation corpus\n",
    "> - âœ…  Understand attention mechanism within the Transformer architecture\n",
    "> - âœ…  Be more interested in NLP ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yDoUnLJKoIK"
   },
   "source": [
    "## Task A: Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Reminder**\n",
    "We will be implementing the following encoder-decoder transformer architecture following the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper!\n",
    "\n",
    "![Transformer Architecture](transformer_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kH-A3gdHF7o"
   },
   "source": [
    "In this part, you will implement an encoder-decoder Transformer model using [Pytorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T14:42:37.478843Z",
     "iopub.status.busy": "2024-02-22T14:42:37.478185Z",
     "iopub.status.idle": "2024-02-22T14:42:50.715331Z",
     "shell.execute_reply": "2024-02-22T14:42:50.714317Z",
     "shell.execute_reply.started": "2024-02-22T14:42:37.478777Z"
    },
    "id": "vFN2GC6PX1_P",
    "outputId": "aa984dba-d338-4f62-eaba-1c8006dc623e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # These layers transform the input embeddings to queries, keys, and values\n",
    "        self.query_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_proj   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Final projection after concatenating heads\n",
    "        self.out_proj   = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Dropout for attention weights\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query, key, value: Tensors of shape [batch_size, seq_len, embed_dim].\n",
    "            mask: Optional tensor of shape [batch_size, 1, seq_len, seq_len] or \n",
    "                  [batch_size, num_heads, seq_len, seq_len] to mask attention.\n",
    "\n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, embed_dim]\n",
    "            attn_weights: [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1) Linear projections of Q, K, V\n",
    "        Q = self.query_proj(query)  # [batch_size, seq_len, embed_dim]\n",
    "        K = self.key_proj(key)      # [batch_size, seq_len, embed_dim]\n",
    "        V = self.value_proj(value)  # [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # 2) Reshape into [batch_size, num_heads, seq_len, head_dim]\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Now Q, K, V each have shape [batch_size, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # 3) Scaled dot-product attention\n",
    "        #    attention_scores = (Q @ K^T) / sqrt(head_dim)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        # attention_scores: [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "        # Optional mask\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax along the last dimension (seq_len of K)\n",
    "        attn_weights = F.softmax(attention_scores, dim=-1)\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Multiply by V\n",
    "        # attention_output shape: [batch_size, num_heads, seq_len, head_dim]\n",
    "        attention_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # 4) Concat heads: transpose back and then reshape\n",
    "        # From [batch_size, num_heads, seq_len, head_dim] -> [batch_size, seq_len, embed_dim]\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        # 5) Final linear projection\n",
    "        output = self.out_proj(attention_output)\n",
    "        \n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.linear1   = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2   = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.norm1     = nn.LayerNorm(embed_dim)\n",
    "        self.norm2     = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1  = nn.Dropout(dropout)\n",
    "        self.dropout2  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # 1) Multi-head self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=mask)\n",
    "        x = x + attn_output  # residual\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # 2) Feed-forward\n",
    "        ff_output = self.linear2(F.relu(self.linear1(x)))\n",
    "        x = x + ff_output  # residual\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn        = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.cross_attn       = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        self.linear1          = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2          = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "        self.norm1            = nn.LayerNorm(embed_dim)\n",
    "        self.norm2            = nn.LayerNorm(embed_dim)\n",
    "        self.norm3            = nn.LayerNorm(embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, tgt_seq_len, embed_dim] (decoder input embeddings)\n",
    "        enc_output: [batch_size, src_seq_len, embed_dim] (encoder output)\n",
    "        tgt_mask: mask for target self-attention\n",
    "        cross_attn_mask: mask for encoder-decoder attention\n",
    "        \"\"\"\n",
    "        # 1) Masked self-attention (decoder)\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=tgt_mask)\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # 2) Cross-attention (encoder-decoder)\n",
    "        cross_attn_output, attn_weights = self.cross_attn(x, enc_output, enc_output, mask=cross_attn_mask)\n",
    "        x = x + cross_attn_output\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # 3) Feed-forward\n",
    "        ff_output = self.linear2(F.relu(self.linear1(x)))\n",
    "        x = x + ff_output\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len=1024, dropout=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = nn.Embedding(max_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        \"\"\"\n",
    "        src: [batch_size, src_seq_len]\n",
    "        src_mask: (optional)\n",
    "        \"\"\"\n",
    "        x = self.embedding(src) * math.sqrt(self.embed_dim)\n",
    "        x = x + self.pos_encoding(torch.arange(src.size(1)).to(src.device)).unsqueeze(0)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        \n",
    "        return x  # shape: [batch_size, src_seq_len, embed_dim]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len=1024, dropout=0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = nn.Embedding(max_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, enc_output, tgt_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        tgt: [batch_size, tgt_seq_len]\n",
    "        enc_output: [batch_size, src_seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.embedding(tgt) * math.sqrt(self.embed_dim)\n",
    "        x = x + self.pos_encoding(torch.arange(tgt.size(1)).to(tgt.device)).unsqueeze(0)\n",
    "\n",
    "        attn_weights = None\n",
    "        for layer in self.layers:\n",
    "            x, attn_weights = layer(x, enc_output, tgt_mask=tgt_mask, cross_attn_mask=cross_attn_mask)\n",
    "\n",
    "        # Final projection to vocabulary\n",
    "        logits = self.out_proj(x)  # [batch_size, tgt_seq_len, vocab_size]\n",
    "        return logits, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(seq_len, device=None):\n",
    "    \"\"\"\n",
    "    Returns a 2D causal mask of shape [seq_len, seq_len], \n",
    "    where True means 'allowed to attend' and False means 'disallowed'.\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=device))\n",
    "\n",
    "def expand_causal_mask(causal_2d, batch_size, num_heads):\n",
    "    # causal_2d: shape [seq_len, seq_len]\n",
    "    # expand to [batch_size, num_heads, seq_len, seq_len]\n",
    "    causal_4d = causal_2d.unsqueeze(0).unsqueeze(0)  # => [1,1,seq_len,seq_len]\n",
    "    causal_4d = causal_4d.expand(batch_size, num_heads, causal_2d.size(0), causal_2d.size(1))\n",
    "    return causal_4d\n",
    "\n",
    "def expand_padding_mask(pad_mask_2d, num_heads):\n",
    "    # pad_mask_2d: shape [batch_size, seq_len], 1 = valid, 0 = pad\n",
    "    # step 1) Convert to bool if needed\n",
    "    pad_mask_bool = pad_mask_2d.bool()  # shape [batch_size, seq_len]\n",
    "    # step 2) unsqueeze => [batch_size, 1, 1, seq_len]\n",
    "    pad_mask_4d = pad_mask_bool.unsqueeze(1).unsqueeze(2)\n",
    "    # step 3) broadcast across the query dimension\n",
    "    batch_size, _, _, seq_len = pad_mask_4d.shape\n",
    "    pad_mask_4d = pad_mask_4d.expand(batch_size, num_heads, seq_len, seq_len)\n",
    "    return pad_mask_4d\n",
    "\n",
    "\n",
    "def build_decoder_mask(\n",
    "    pad_mask_2d: torch.Tensor, \n",
    "    num_heads: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine the target padding mask with the causal mask \n",
    "    to produce a final decoder mask of shape:\n",
    "    [batch_size, num_heads, seq_len, seq_len], \n",
    "    where True means 'allowed', False means 'masked out'.\n",
    "    \"\"\"\n",
    "    device = pad_mask_2d.device\n",
    "    batch_size, seq_len = pad_mask_2d.shape\n",
    "\n",
    "    # 1) Build the 2D causal mask\n",
    "    causal_2d = build_causal_mask(seq_len, device=device)\n",
    "\n",
    "    # 2) Expand to 4D\n",
    "    causal_4d = expand_causal_mask(causal_2d, batch_size, num_heads)\n",
    "\n",
    "    # 3) Expand the padding mask to 4D\n",
    "    pad_4d = expand_padding_mask(pad_mask_2d, num_heads)\n",
    "\n",
    "    # 4) Final mask = causal AND pad\n",
    "    final_mask = causal_4d & pad_4d  # shape [batch_size, num_heads, seq_len, seq_len]\n",
    "    return final_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim=512, num_heads=8, hidden_dim=2048, \n",
    "                 num_layers=6, dropout=0):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.encoder = Encoder(src_vocab_size, embed_dim, num_heads, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, embed_dim, num_heads, hidden_dim, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, cross_attn_mask=None):\n",
    "\n",
    "        if src_mask is not None:\n",
    "            src_mask = src_mask[:, None, None, :]\n",
    "            src_mask = src_mask.expand(-1, -1, src_mask.size(-1), -1)\n",
    "\n",
    "        tgt_mask = build_decoder_mask(tgt_mask, num_heads=self.num_heads)\n",
    "\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        logits, attn_weights = self.decoder(tgt, enc_output, tgt_mask, cross_attn_mask)\n",
    "        return logits, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B: Train a Machine Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/u14157_ic_nlp_001_files_nfs/nlpdata1/home/bkhmsi/miniconda3/envs/bkhmsi-brainscore/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_from_disk(\"wmt14_fr_en\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a BPE Tokenizer\n",
    "\n",
    "> You will learn about BPE tokenization later in the semester. Here we provide you with the code necessary to train your own tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "def batch_iterator(dataset, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Yields batches of text from the Hugging Face dataset.\n",
    "    Args:\n",
    "        dataset: a huggingface dataset split (e.g. train_dataset)\n",
    "        batch_size: how many samples per batch\n",
    "        text_column: name of the column containing the text\n",
    "    \"\"\"\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        # Extract a batch of examples\n",
    "        batch = dataset[i : i + batch_size]\n",
    "\n",
    "        # 'batch' is now a list of strings (if text_column is indeed text).\n",
    "        yield batch\n",
    "\n",
    "\n",
    "fr_training_set = [dataset[\"train\"][i]['translation']['fr'] for i in range(100_000)]\n",
    "en_training_set = [dataset[\"train\"][i]['translation']['en'] for i in range(100_000)]\n",
    "training_set = en_training_set + fr_training_set\n",
    "\n",
    "# 1) Initialize tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# 2) Train from the iterator\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=batch_iterator(training_set, batch_size=1000),\n",
    "    vocab_size=32_000,  # Choose your vocab size\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]  # Or any set of special tokens you prefer\n",
    ")\n",
    "\n",
    "# 3) Save the tokenizer\n",
    "tokenizer.save(\"my_bytelevel_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 439, 8722, 6667, 1727,  291, 3255]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "['The', 'Ä quick', 'Ä bro', 'wn', 'Ä f', 'ox']\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"my_bytelevel_tokenizer.json\",  # or the two files from the BPE approach\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\"\n",
    ")\n",
    "\n",
    "encoded_input = hf_tokenizer(\"The quick brown fox\", return_tensors=\"pt\", add_special_tokens=True)\n",
    "decoded_output = hf_tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n",
    "\n",
    "print(encoded_input)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "max_len = 256\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Model parameters\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "hidden_dim = 2048\n",
    "num_layers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Transformer(\n",
    "    src_vocab_size=hf_tokenizer.vocab_size, \n",
    "    tgt_vocab_size=hf_tokenizer.vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"> Number of parameters in the model: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "num_batches = int(np.ceil(len(dataset['train']) / batch_size))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=hf_tokenizer.pad_token_id)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch_num in range(epochs):\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "\n",
    "        train_batch = dataset['train'][batch_idx * batch_size: (batch_idx + 1) * batch_size]['translation']\n",
    "\n",
    "        src_train = [train_batch[i]['fr'] for i in range(len(train_batch))]\n",
    "        tgt_train = [train_batch[i]['en'] for i in range(len(train_batch))]\n",
    "        \n",
    "        src_tokens = hf_tokenizer(src_train, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "        tgt_tokens = hf_tokenizer(tgt_train, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "\n",
    "        # Prepend BOS token to target\n",
    "        tgt_tokens[\"input_ids\"] = torch.cat([\n",
    "            torch.full_like(tgt_tokens[\"input_ids\"][:, :1], hf_tokenizer.bos_token_id), \n",
    "            tgt_tokens[\"input_ids\"][:, :-1]\n",
    "        ], dim=1)\n",
    "\n",
    "        logits, _ = model(\n",
    "            src=src_tokens['input_ids'],\n",
    "            tgt=tgt_tokens[\"input_ids\"],\n",
    "            src_mask=src_tokens[\"attention_mask\"],\n",
    "            tgt_mask=tgt_tokens[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        # shift labels to the right\n",
    "        labels = tgt_tokens[\"input_ids\"][:, 1:].contiguous()\n",
    "        logits = logits[:, :-1].contiguous()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch_num}, Batch {batch_idx}/{num_batches}, Loss: {loss.item()}\", end='\\r')\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # if (batch_idx+1) % 200 == 0:\n",
    "        #     print(\"Saving model...\")\n",
    "        #     torch.save(model.state_dict(), \"transformer_model.pt\")\n",
    "    \n",
    "    print(f\"Epoch {epoch_num}, Loss: {np.mean(train_losses):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"transformer_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Loss')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAG4CAYAAAB1v/zVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+kklEQVR4nO3deXxU9b3/8fckmQlZyEpkSQLZCiYhEJUYcUEltAhY8YK9Yl1LVSoqFcutaKvW1p+1Xtu6AV5Ra22ttm61FeqGVIUiRQUMJIjZIATBELInJJNkfn+EpAxJIMucObO8no/HfVi+c+acTz83JW+/3+85x+JwOBwCAAAwUYDZBQAAABBIAACA6QgkAADAdAQSAABgOgIJAAAwHYEEAACYjkACAABMRyABAACmI5AAAADTEUgA9Gr58uWaPn36oL77+OOPa8KECS6uCIAvCzK7AAAD099f9M8//7xyc3MNrsbzLF++XG+//ba2bt1qdikABsDCu2wA7/LGG2/0+PPGjRv10EMPOY2fc845GjFixKCvY7fb5XA4ZLPZBvzdtrY2tbe3Kzg4eNDXHywCCeCdmCEBvMzcuXOd/rx9+3Zt3Lixx/jxmpubFRIS0u/rWK3WQdUnSUFBQQoK4q8XAP3HHhLAB1199dW6+OKLtWPHDl155ZWaPHmyfvOb30iS3nvvPd14440699xzNXHiRM2YMUMrVqxQe3u70zmO30Oyb98+TZgwQc8884z+/Oc/a8aMGZo4caLmz5+vzz//3Om7ve0hmTBhgn7+85/rvffe08UXX6yJEydqzpw5+vDDD3vUv3nzZs2bN09ZWVmaMWOGXnrpJZfvS/nHP/6hefPmadKkScrNzdWyZct08OBBp2MqKyt15513atq0aZo4caLOPfdc3XTTTdq3b1/3Mfn5+fr+97+v3NxcTZo0SdOnT9edd97psjoBf8G/wgA+qqamRjfccIPmzJmjSy65RLGxsZKk119/XaGhofre976n0NBQffzxx3rsscfU0NCgO+6446TnffPNN9XY2KjLL79cFotFTz/9tG699Va99957J51V+fTTT/XOO+/ou9/9rsLCwvSHP/xBS5Ys0fr16xUdHS1JKigo0PXXX6+4uDjdeuut6ujo0IoVKxQTEzP0phz12muv6c4771RWVpZuv/12VVVV6fnnn9dnn32mv/71r4qIiJAk3XrrrSoqKtJVV12l+Ph4HT58WBs3btRXX32lhIQEVVVV6fvf/76io6N14403KiIiQvv27dO7777rsloBf0EgAXxUZWWl7rvvPi1YsMBp/Ne//rWGDRvW/ecrrrhC99xzj1588UUtXbr0pHtG9u/fr3feeUeRkZGSpOTkZC1evFgbNmzQhRdeeMLvFhcXa+3atRo7dqwkKTc3V3PnztWaNWt01VVXSZIee+wxBQYG6sUXX9TIkSMlSbNmzdLs2bMH1oA+2O12Pfzwwxo/frxeeOGF7n0uZ5xxhhYtWqTnnntOS5YsUV1dnbZu3aof//jH+v73v9/9/UWLFnX/561bt6q2tlbPPPOMsrKyuseXLl3qkloBf8KSDeCjbDab5s2b12P82DDS0NCgw4cPa8qUKWpublZJSclJzzt79uzuMCJJU6ZMkSSVl5ef9Ltnn312dxiRpFNPPVXh4eHd321vb9emTZuUl5fXHUYkady4cTrvvPNOev7+2LFjh6qqqnTFFVc4bbq94IILlJKSon/+85+SOvtktVr173//W7W1tb2ea/jw4ZKkf/7zn7Lb7S6pD/BXzJAAPmrkyJG9znZ8+eWXeuSRR/Txxx+roaHB6bP6+vqTnnf06NFOf+4KJ3V1dQP+btf3u75bVVWlI0eOaNy4cT2O621sMPbv3y+pc2bneCkpKfr0008ldQa6ZcuW6Ve/+pXOOeccTZ48WRdccIEuvfRSxcXFSZLOPPNMzZw5U0888YSee+45nXnmmZoxY4a+/e1vD+ruJMCfMUMC+KhjZ0K61NXV6aqrrtKuXbu0ZMkSPfnkk/rd736nZcuWSZI6OjpOet7AwMBex/vzBIGhfNcM1113nd5++23dfvvtCg4O1qOPPqrZs2eroKBAkmSxWPTYY4/pz3/+s6666iodPHhQd911l+bNm6fGxkaTqwe8C4EE8CP//ve/VVNTowcffFDXXnutLrzwQp199tlOSzBmio2NVXBwsPbs2dPjs97GBmPMmDGSpNLS0h6flZaWdn/eZezYsVq4cKGeffZZvfnmm7Lb7Xr22WedjsnOztbSpUv12muv6eGHH9aXX36ptWvXuqRewF8QSAA/EhDQ+T/5Y2ckWltb9ac//cmskpwEBgbq7LPP1rp165xuwd2zZ48++ugjl1xj4sSJio2N1UsvvaTW1tbu8Q8++EDFxcW64IILJHU+t6WlpcXpu2PHjlVYWFj392pra3vM7qSnp0uS07kBnBx7SAA/ctpppykyMlLLly/X1VdfLYvFojfeeMOjlkxuueUWbdiwQVdccYWuuOIKdXR06I9//KO+8Y1vqLCwsF/nsNvtWrlyZY/xyMhIXXnllVq2bJnuvPNOXXXVVZozZ073bb/x8fG67rrrJEllZWW67rrrdNFFFyktLU2BgYF67733dOjQIc2ZM0dS5y3UL774ombMmKGxY8eqsbFRf/nLXxQeHq5p06a5rCeAPyCQAH4kOjpaTz75pH71q1/pkUceUUREhC655BJNnTrV6dZWM02cOFGrV6/WQw89pEcffVSjR4/WkiVLVFJS0q+7gKTOQPLoo4/2GB87dqyuvPJKzZs3T8OGDdPq1av18MMPKzQ0VDNmzND//M//dD+DZNSoUZozZ442bdqkv/3tbwoMDFRKSooeeeQRzZw5U1Lnptb8/HytXbtWhw4d0vDhwzVp0iQ9/PDDSkxMdF1TAD/Au2wAeIXFixerqKhI77zzjtmlADAAe0gAeJwjR444/bmsrEwffvihzjzzTJMqAmA0lmwAeJwZM2bov/7rv5SYmKiKigq99NJLslqtuv76680uDYBBCCQAPM55552nNWvWqLKyUjabTdnZ2br99tuVlJRkdmkADMIeEgAAYDr2kAAAANMRSAAAgOm8Yg/J1q1b5XA4ZLVazS4FAAD0k91ul8Vi0WmnnXbSY71ihsThcBj2JEmHw6HW1laPelKlr6LX7kOv3Ydeuw+9dh9X9Xogv7+9Yoaka2YkKyvL5eduampSYWGh0tLSFBoa6vLz4z/otfvQa/eh1+5Dr93HVb3Oz8/v97FeMUMCAAB8G4EEAACYjkACAABMRyABAACmI5AAAADTEUgAAIDpCCQAAMB0BBIAAGA6AgkAADAdgQQAAJiOQAIAAEzn94EkMDBQo0aNUmBgoNmlAADgt/w6kLTa21VxqFlbS1tUceiIWu3tZpcEAIBf8oq3/Rqh1d6uFa9s1/uflHeP5eUkavH8ybJZmS0BAMCd/HaGpKKywSmMSNK6LeWqqGwwqSIAAPyX3waSgtLDvY4X9jEOAACM47eBJCM5ptfx9D7GAQCAcfw2kMTHhSsvJ9FpLC8nUfFx4SZVBACA//LbTa02a6AWz5+si6YmafeeamUkx2jsqAg2tAIAYAK/nSGROkPJF2VVqmloUWV1E2EEAACT+HUgkaSvDjXo5XVfKr+40uxSAADwW34fSCLDbJKk2oZWkysBAMB/EUjCgyURSAAAMJPfB5Ko4Z0zJDUNLSZXAgCA//L7QMKSDQAA5vP7QBI1/OiSTWOr2jscJlcDAIB/8vtAEhFqlSQ5HFJ9I7MkAACYwe8DSWBggEKDO9tQXX/E5GoAAPBPfh9IJCl8WGcbaurZ2AoAgBkGHEj27Nmje+65R3PnzlVGRoYuvvjiXo97+eWXNXPmTGVlZemSSy7R+vXrh1ysUcKGdT6hlTttAAAwx4ADyZdffqkPPvhA48aNU2pqaq/HrFmzRnfffbdmzZql1atXKzs7W7fccou2bds21HoNER7SGUiq6wgkAACYYcAv15s+fbpmzJghSVq+fLl27NjR45jHHntMc+bM0W233SZJOuuss7R7926tWLFCq1evHlrFBuhesmGGBAAAUwx4hiQg4MRfKS8vV1lZmWbNmuU0Pnv2bG3atEmtrZ53J0v40SUbNrUCAGAOl29qLSkpkSQlJyc7jaempsput6u8vNzVlxyyMDa1AgBgqgEv2ZxMbW2tJCkiIsJpvOvPXZ8PlMPhUFNT09CK60Vzc3P3DMnh2mZDroFOzc3NTv+Ecei1+9Br96HX7uOqXjscDlksln4d6/JAYhS73a7CwkJDzh0e0jlDUlXbZNg18B9lZWVml+A36LX70Gv3odfu44pe22y2fh3n8kASGRkpSaqvr1dcXFz3eF1dndPnA2W1WpWWljb0Ao/T3Nys+uZiSVJTS4cmTDhVAQH9S3MYmObmZpWVlSkpKUkhISFml+PT6LX70Gv3odfu46peFxUV9ftYlweSlJQUSZ17Sbr+c9efrVarEhMTB3Vei8Wi0NBQl9R4vLDgAFksnY+Pt3cEKjp8mCHXQaeQkBDD/n8JZ/Tafei1+9Br9xlqr/u7XCMZsKk1MTFRSUlJeuutt5zG165dq6lTp/Z76sadAgIsigjtrItbfwEAcL8Bz5A0Nzfrgw8+kCRVVFSooaGhO3yceeaZiomJ0a233qply5Zp7Nixys3N1dq1a/X555/rj3/8o2urd6HIcJtqG1tVXdei5DFmVwMAgH8ZcCCpqqrSD3/4Q6exrj8///zzys3N1cUXX6zm5matXr1aTz31lJKTk/XEE0/otNNOc03VBogKt2nvQammgWeRAADgbgMOJAkJCfriiy9Oetx3vvMdfec73xlUUWaIDA+WxLNIAAAwA2/7PSoqvHMPSTWBBAAAtyOQHBV5NJAwQwIAgPsRSI6K7J4hYQ8JAADuRiA5Koo9JAAAmIZAclT3kg3PIQEAwO0IJEd1zZDUNbaqvb3D5GoAAPAvBJKjhodaFXD08fG1ja1mlwMAgF8hkBwVEGDpfhZJdR0bWwEAcCcCyTGihh/d2Mo+EgAA3IpAcgzutAEAwBwEkmNERwyTxNNaAQBwNwLJMZghAQDAHASSY0RHHN3UytNaAQBwKwLJMZghAQDAHASSY3TdZcMeEgAA3ItAcozo4Z2bWpkhAQDAvQgkx+iaIalvalUbj48HAMBtCCTHGB5qU0CARZJUy8PRAABwGwLJMQICLIo6+tZf9pEAAOA+QWYX4GlGRIYqM2WEQmy0BgAAd+G37nHuuT5XFZUN2rr7a7W2tSs+Llw2a6DZZQEA4NMIJMdotbfr2b/v1PuflHeP5eUkavH8yYQSAAAMxB6SowIDA1VR2eAURiRp3ZZyVVQ2mFQVAAD+gUBylMViUUHp4V4/K+xjHAAAuAaB5CiHw6GM5JheP0vvYxwAALgGgeSo9vbODax5OYlO43k5iYqPCzepKgAA/AObWo9hswZq8fzJuuisJO3eW62MlFiNHTmcDa0AABiMGZLj2KyBKtpXo5qGFn19uIkwAgCAGxBIemFv69DL677UB1v3mV0KAAB+gUDSi5QxkZKk0oo6kysBAMA/EEh6kTQmQpL0VVWjmo7YTa4GAADfRyDpRWR4sEZEDpMkle5nlgQAAKMRSPqQHH902WZ/rcmVAADg+wgkfejaR1JSQSABAMBoBJI+MEMCAID7EEj60DVDsudAvdraO0yuBgAA30Yg6cPImFCFBAfJ3tahiq952y8AAEYikPQhIMCi5KO3/5awbAMAgKEIJCfAxlYAANyDQHICbGwFAMA9CCQn8J8Zkjo5HA6TqwEAwHcRSE5g7KjhCgiwqL6pVVW1R8wuBwAAn0UgOQGbNVCJp4RLYmMrAABGCjK7AE+XlhilsaMiNMwWaHYpAAD4LALJSVx/yUTtPViv4n21Gh5qU3xcuGxWwgkAAK5EIDmBVnu7Vr+xQ+9/Ut49lpeTqMXzJxNKAABwIfaQnEBFZYNTGJGkdVvKVVHJk1sBAHAlAskJFJQe7nW8sI9xAAAwOASSE8hIjul1PL2PcQAAMDgEkhOIjwtXXk6i01heTqLi48JNqggAAN9k2KbWdevW6cknn1RRUZHCwsJ0xhlnaNmyZUpMTDz5lz2EzRqoxfMna+60VOUXHVJaYpRS4iPZ0AoAgIsZMkOyefNm3XLLLUpLS9OKFSt01113adeuXVq4cKGOHPGuJ57arIFKHhOpkOAgvbmhVO9u3mt2SQAA+BxDZkjWrFmjMWPG6IEHHpDFYpEkxcTE6Nprr9WOHTs0ZcoUIy5rqMYjdn20rUINTa369nkpZpcDAIBPMWSGpK2tTWFhYd1hRJKGDx8uSV77krozTh0pSdpRUqUjrW0mVwMAgG8xJJDMmzdPxcXFeuGFF1RfX6/y8nL95je/UUZGhk4//XQjLmm4hFPCdUp0iOxtHcovOmR2OQAA+BRDlmymTJmiJ554Qj/60Y/085//XJKUnp6up59+WoGBg9sQ6nA41NTU5MoyJUnNzc1O/zyRSWkxem9LhTbv2K/MpAiX1+LrBtJrDA29dh967T702n1c1WuHw+G0WnIiFocBayifffaZFi1apPnz5+uCCy5QTU2NVq5cqaCgIP3pT3/SsGHDBnS+/Px8tba2urrMAdu1r1kvfVil6PBALfn2qH43GQAAf2Wz2ZSVlXXS4wyZIbn//vt11llnafny5d1j2dnZuuCCC/TGG2/o8ssvH/A5rVar0tLSXFmmpM70V1ZWpqSkJIWEhJzw2KSUNr288Z+qbmhX1CnjNGZEmMvr8WUD6TWGhl67D712H3rtPq7qdVFRUb+PNSSQFBcXKy8vz2ls1KhRio6O1t69g7tt1mKxKDQ01BXl9SokJOSk5w8NlTKTY/V50SEV7KlT2tg4w+rxZf3pNVyDXrsPvXYfeu0+Q+31QFYSDNnUOmbMGBUUFDiNVVRUqLq6WvHx8UZc0m3OOHWkrEEBamltN7sUAAB8hiEzJAsWLNADDzyg+++/X9OnT1dNTY1WrVql2NhYzZo1y4hLus3UrFE6//QEHTzcqDUbS5WRHKP4uHCe3goAwBAYEkiuueYa2Ww2vfjii3r11VcVFham7OxsPfLII4qOjjbikm4TGxmiFa9s1/uflHeP5eUkavH8yYQSAAAGyZBAYrFYdMUVV+iKK64w4vSmqqhscAojkrRuS7nmTktV8phIk6oCAMC78bbfASooPdzreGEf4wAA4OQIJAOUkRzT63h6H+MAAODkCCQDFB8XrrycRKexvJxExceFm1QRAADez5A9JL7MZg3U4vmTdfE5KSoordL4sdFKiY9kQysAAEPADMkg2KyBij8lXLWNrfrbRyWqqj1idkkAAHg1AskghQQHaVfZYX20rUIf7/jK7HIAAPBqBJIhmJo1WpIIJAAADBGBZAhyMzsDSWHZYVXXs2wDAMBgEUiGIC46RGmJUXI4pH/vPGB2OQAAeC0CyRCdNXGUJOnjHQQSAAAGi0AyRFMndi7bbNtdqaYjdpOrAQDAOxFIhihx5HCNHTlcU7NG6+vqJrPLAQDAK/FgtCGyWCx68JZzVX6wXvlFVXI4Op/myoPSAADoPwLJELXa2/X0Gzuc3gCcl5OoxfMnE0oAAOgnlmyGqKKywSmMSNK6LeWqqGwwqSIAALwPgWSICkoP9zpe2Mc4AADoiUAyRBnJMb2Op/cxDgAAeiKQDFF8XLjychKdxvKmJCo+LtykigAA8D5sah0imzVQi+dP1txpqcovPqS0hCiNGx3BhlYAAAaAGRIXsFkDlTwmUvFx4XpzQ6meeWOH2SUBAOBVmCFxoaCAAH20rUIxEcFyOByyWCxmlwQAgFdghsSF0pNjZAsK0OG6Fu09UG92OQAAeA0CiQvZrIGamDpCkrR1d6XJ1QAA4D0IJC6WPT5OkrR199cmVwIAgPcgkLjYaRNOkSTtKK6Sva3d5GoAAPAOBBIXGzdquKKHB6vV3t7nU1wBAIAzAomLWSyW7mWbbewjAQCgXwgkBsgef4qsQbQWAID+4remAc449RStvuubyskYqTUbS1W6v1atdvaTAADQFx6MZoCQ4CCteGW73v+kvHssLydRi+dP5pHyAAD0ghkSA1RUNjiFEUlat6VcFZUNJlUEAIBnI5AYoK+7awq56wYAgF4RSAyQkRzT63h6H+MAAPg7AokB4uPClZeT6DSWl5Oo+LhwkyoCAMCzsanVADZroBbPn6zZZydrV9lhnZoUo6TREWxoBQCgD8yQGMRmDZTFItU0tOijbRWEEQAAToBAYqCRMWF6ed2X+usHxaptaDG7HAAAPBaBxEARYTaNHTVcklRQWmVyNQAAeC4CicEyU2IlSTtKCCQAAPSFQGKwiUcDyU4CCQAAfSKQGKxrhqS0olZNR+wmVwMAgGcikBgsNjJEo2JD1eGQCst4UisAAL0hkLhBJss2AACcEIHEDTKTj25sLSaQAADQGwKJG2SmdgaSL8tr1GJvN7kaAAA8D4HEDUbHhikmIlht7R3avbfa7HIAAPA4vMvGDSwWiyalxam9wyGHw2F2OQAAeBwCiZv8YN4k7TlQp+J9NRoealN8XDjvtwEA4CgCiRu02tv11F/z9f4n5d1jeTmJWjx/MqEEAACxh8QtKiobnMKIJK3bUq6KygaTKgIAwLMQSNygoLT3B6IV9jEOAIC/MTSQvP7667r00kuVlZWl3NxcXX/99Tpy5IiRl/RIGckxvY6n9zEOAIC/MWwPyapVq7R69Wr94Ac/UHZ2tqqrq7Vp0ya1t/vfczji48KVl5OodVuc95DEx4WbWBUAAJ7DkEBSUlKiJ554QitXrtT555/fPT5z5kwjLufxbNZALZ4/WXOnpSq/6JDSEqOUMiaSDa0AABxlyJLNa6+9poSEBKcw4u9s1kAlj4nUsOAgvbmhVB98ts/skgAA8BiGBJLt27dr/PjxWrlypaZOnaqJEydqwYIF2r59uxGX8yqV1c36aFuF8nmvDQAA3QxZsqmsrNSOHTu0e/du3XvvvQoJCdGTTz6phQsX6p133lFsbOyAz+lwONTU1OTyWpubm53+abTU+DBJ0o6SQ4b89/Fk7u61P6PX7kOv3Ydeu4+reu1wOGSxWPp1rMVhwLPMZ86cqbKyMr3xxhs69dRTJUk1NTWaPn26rr32Wv3whz8c0Pny8/PV2trq6jJN0WLv0IOv7JfDId02d5Siwng2HQDAd9lsNmVlZZ30OEN+G0ZERCgqKqo7jEhSVFSUMjIyVFRUNKhzWq1WpaWluarEbs3NzSorK1NSUpJCQkJcfv7epPyrQcUVdWoLGqH09NFuuaYnMKPX/opeuw+9dh967T6u6vVAfucbEkjS0tK0d+/eXj9raWkZ1DktFotCQ0OHUtYJhYSEGHr+Y2Wlxam4ok5fVtRr5tmpbrmmJ3Fnr/0dvXYfeu0+9Np9htrr/i7XSAZtar3wwgtVU1OjwsLC7rHq6mrt3LlTmZmZRlzSq2SmdO6hKShlYysAAJJBMyQzZsxQVlaWlixZoqVLlyo4OFhPPfWUbDabvvvd7xpxSa+SkdwZSMoPNqi2oUWR4cEmVwQAgLkMmSEJCAjQU089pezsbN1zzz26/fbbFR4erhdeeEFxcXFGXNKrRITZlDhyuKS+33MDAIA/MewWj5iYGP3v//6vUaf3epkpsSo/WK+C0ipNzfKfja0AAPSGt/2aJPPoi/V2lrCPBAAAAolJMlJiZQ0K0JgRYbK3+d8LBwEAOBZP5TLJKdGheuYn39RXVY16++M9ykyJVXxcOC/cAwD4JQKJSVrt7XpuTYHe/6S8eywvJ1GL508mlAAA/A5LNiapqGxwCiOStG5LuSoqG0yqCAAA8xBITNLX7b6F3AYMAPBDBBKTZBy9y+Z46X2MAwDgywgkJomPC1deTqLTWF5OouLjwk2qCAAA87Cp1SQ2a6AWz5+sOeckq7D0sCaMi1bymEg2tAIA/BIzJCayWQMVPXyYahpatGZjmQbwUkQAAHwKgcRksZHD9O7mvVr/abl2760xuxwAAExBIDGZxWLRxNTOt//mFx8yuRoAAMxBIPEAWWkjJEn5RQQSAIB/IpB4gKzUzkCyq+ww77UBAPglAokHSDglXFHDg9Xa1sE+EgCAXyKQeACLxdI9S8I+EgCAPyKQeIisro2t7CMBAPghAomHmHjMPpJWO/tIAAD+hUDiIZz3kVSbXQ4AAG5FIPEQzvtIqkyuBgAA9+JdNh5k8tHnkYyIHGZyJQAAuBczJB7k/NMTdPG5yWpuaVPp/lr2kgAA/AYzJB6i1d6uVa99rvc/Ke8ey8tJ1OL5k3kDMADA5zFD4iEqKhucwogkrdtSrorKBpMqAgDAfQgkHqKg9HCv44V9jAMA4EsIJB4iIzmm1/H0PsYBAPAlBBIPER8XrrycRKex6VMSFR8XblJFAAC4D5taPYTNGqjF8ydr7rRU7SiuUmpCpBJPGc6GVgCAX2CGxIPYrIFKHhOpyeNH6M0NpVq+YoPZJQEA4BbMkHig2IgQbfx8vzo6HPr6cJNOiQk1uyQAAAzFDIkHCguxasLYaEnS1t2VJlcDAIDxCCQeKnt8nCRp2+6vTa4EAADjEUg8VFcg2f5lpdo7HCZXAwCAsQgkHmr82GiFDgtSfZNdJRU1ZpcDAIChCCQeKigwQFmpnW//3cY+EgCAjyOQeLDTuveREEgAAL6NQOLBsiecImtQgGIihsne1m52OQAAGIbnkHiwMSPC9MxPvqmvqhr19sd7lJkSq/i4cJ7eCgDwOQQSD2Zv69Bzawr0/ifl3WN5OYlaPH8yoQQA4FNYsvFgFZUNTmFEktZtKVdFZYNJFQEAYAwCiQcrKD3c63hhH+MAAHgrAokHy0iO6XU8vY9xAAC8FYHEg8XHhSsvJ9FpLC8nUfFx4SZVBACAMdjU6sFs1kAtnj9Z3z43RTtLqvSNxGilJkSyoRUA4HOYIfFwNmugksdEqKmlTX/fUKLS/bVmlwQAgMsRSLxAQECAyr6q00fbKvTpLt7+CwDwPQQSL3HGhFMkSZ8RSAAAPohA4iVOP7UzkOwur1ZdY6vJ1QAA4FoEEi8RGxmipNERcjikbbuZJQEA+BYCiRc5/eiyDftIAAC+xvBA0tjYqGnTpmnChAnKz883+nI+rWvZ5rMvvlZHh8PkagAAcB3DA8nKlSvV3t5u9GX8QkZyjIbZAlVT36Kyr+rMLgcAAJcxNJAUFxfrT3/6k2699VYjL+M3rEGBmpQWJ0n6dNdBk6sBAMB1DA0k999/vxYsWKDk5GQjL+NXTj/1FFmDAtTW3mF2KQAAuIxhj45/6623tHv3bj3++OPauXOnUZfxO7mZo3TWxNE6eLhRazaWKiM5RvFx4TxOHgDg1QwJJM3NzXrwwQe1dOlShYe75kVwDodDTU1NLjnXsZqbm53+6emGh9q08tXP9f4n5d1jeTmJumneJLW3efbzSbyt196MXrsPvXYfeu0+ruq1w+GQxWLp17GGBJJVq1YpNjZW8+fPd9k57Xa7CgsLXXa+45WVlRl2bleJjo5WhzXKKYxI0rot5fr2OUmy2GtUXV1tUnX95w299hX02n3otfvQa/dxRa9tNlu/jnN5IKmoqNCzzz6rFStWqL6+XpK6ZzaamprU2NiosLCwAZ/XarUqLS3NpbVKnemvrKxMSUlJCgkJcfn5XSkoKEjvbtnX62e79tbqmzkJGjVqlJur6j9v6rW3o9fuQ6/dh167j6t6XVRU1O9jXR5I9u3bJ7vdrhtvvLHHZ9dcc40mT56sv/zlLwM+r8ViUWhoqCtK7FVISIih53eVjOTYPsZjZLPZ+p1EzeQtvfYF9Np96LX70Gv3GWqv+7tcIxkQSNLT0/X88887jRUWFuqXv/yl7rvvPmVlZbn6kn4lPi5ceTmJWrfFeQ9JfJxr9uoAAGAGlweSiIgI5ebm9vpZZmamMjMzXX1Jv2KzBmrx/Mm6+JwUFZRWafzYaKXER3KXDQDAq/EuGy9kswZq9Igw1Ta06G8flai+ybPvrgEA4GQMew7JsXJzc/XFF1+441J+IyzEqu1Fh/TFnmpNShuhi6YmmV0SAACDxgyJFzszo/OOmn8XHDC5EgAAhoZA4sXOzOwMJNt3V+pIa5vJ1QAAMHgEEi82btRwxUWHqLWtQ59/ecjscgAAGDQCiRezWCws2wAAfAKBxMt1BZItBQfU0eEwuRoAAAaHQOLlstJiFRFmVWbKCFXXHTG7HAAABsUtt/3CONagQK38cZ4qKhv0r/yvNDE1VvFx4TwoDQDgVQgkXq7V3q5n/77T6Q3AeTmJWjx/MqEEAOA1WLLxchWVDU5hRJLWbSlXRWWDSRUBADBwBBIvV1B6uNfxwj7GAQDwRAQSL5eRHNPreHof4wAAeCICiZeLjwtXXk6i01heTqLi48JNqggAgIFjU6uXs1kDtXj+ZM2dlqr84kNKS4jSuFERbGgFAHgVAokPsFkDlTwmUvWNrXpzQ6ns7e36yXW5ZpcFAEC/EUh8SOTwYH20rUJBgQFqOmJX6DCr2SUBANAv7CHxIWNHDld8XJja2jv0aeHXZpcDAEC/EUh8iMVi0dSsMZKkf+XvN7kaAAD6j0DiY6ZmjZYkfbrroFrt7SZXAwBA/xBIfMw3EqM0InKYmlvate3LSrPLAQCgXwgkPsZiseisrNGyBgWoqqbZ7HIAAOgX7rLxQdOnJOqy6eN18HCj1mwsVUZyDG8ABgB4NAKJDxo3KkIrXtnOG4ABAF6DJRsfxBuAAQDehkDig3gDMADA2xBIfBBvAAYAeBsCiQ/iDcAAAG/DplYf1PUG4G+fm6KdJVX6RmKUUhOi2NAKAPBYzJD4qK43ALe2tevvG0q1dTfvtgEAeC5mSHxYQIBFtQ2t+mhbhSQpN3O0yRUBANA7Zkh83LTT4iVJm3ceUHNLm8nVAADQOwKJj0tLiNLoEWFqtbdr884DZpcDAECvCCQ+zmKxaFp25yzJh1v3mVwNAAC9I5D4gWmnxcsaFKCwYUFqaWXZBgDgedjU6gfGjorQsz/9pvYfatS7/96rzJRYXrYHAPAoBBI/0Gpv1+/eLOBlewAAj8WSjR/gZXsAAE9HIPEDvGwPAODpCCR+gJftAQA8HYHED/T6sr0pvGwPAOA52NTqB7petjd3Wqp2FFcpNSFSCadwlw0AwHMwQ+Inul62d/ak0XpzQ6mu/3/vqbahxeyyAACQRCDxO7GRIdp/qEHNLW1a/2n5yb8AAIAbEEj80MzccZKktz/eI4fDYXI1AAAQSPzS+acnKNgWqH1fN/R5SzAAAO5EIPFDocOsmpbd+X6bsq/qzC4HAADusvFX356WoisvStfBw41as7FUGckxvN8GAGAaAomfih8RrhWvbOf9NgAAj8CSjZ/i/TYAAE9CIPFTvN8GAOBJCCR+ivfbAAA8CYHET/F+GwCAJzFkU+s//vEP/e1vf9POnTtVV1encePG6eqrr9b8+fNlsViMuCQGqPf32wxnQysAwBSGBJLnnntO8fHxWr58uaKjo/Wvf/1Ld999tw4cOKBbbrnFiEtiELrebzM81KZn/75TnxQe0Ko78hQbGWJ2aQAAP2NIIFm1apViYv6zF2Hq1KmqqanR7373Oy1evFgBAawUeZIRUSGqqm1Wc0u73txQqmvnZJhdEgDAzxiSDI4NI13S09PV0NCgpqYmIy6JIbr0/DRZgwJU09Aie1u72eUAAPyM2x6M9umnn2rkyJEKDx/cpkmHw2FImGlubnb6p7+anBqlZ376TX11qFFvf7xHGcmxio8LU0e73WUv4KPX7kOv3Ydeuw+9dh9X9drhcPR776jF4YbXvX7yySe6+uqrdccdd+i6664b8Pfz8/PV2trq+sIgSbJYLDo1PVP/99edTg9Lmz4lUYsuzdSuwp28FRgAMCg2m01ZWVknPc7wGZIDBw5o6dKlys3N1TXXXDPo81itVqWlpbmwsk7Nzc0qKytTUlKSQkL8czNnYGCgKg4d6fHk1vc/Kdel56cqMzNT7e1DX8ah1+5Dr92HXrsPvXYfV/W6qKio38caGkjq6up0ww03KCoqSo8//viQNrNaLBaFhoa6sDpnISEhhp7f0xWW7e99vPSwkscku/Ra/t5rd6LX7kOv3Ydeu89Qez2QR30YFkiOHDmiRYsWqb6+Xn/+8581fPhwoy4FF+DJrQAAMxlyl01bW5tuu+02lZSU6Omnn9bIkSONuAxcqLcnt06fkqgxI3hyKwDAeIbMkNx3331av369li9froaGBm3btq37s4yMDNlsNiMuiyE49smthaWHlRIfqbjoUK3/dK8umuraJRsAAI5nSCDZuHGjJOnBBx/s8dm6deuUkJBgxGUxRF1Pbk0eE6ldew7rhgfeVUhwoM7LTlBYiNXs8gAAPsyQQPL+++8bcVq40YSx0Tpn8hhdNydT+76uV9G+WmUkxyg+Lpz33QAAXM5tD0aDd7FYLLpp3iT93+v5TrcD5+UkavH8yYQSAIBL8VIZ9Ong4aYezyZZt6VcFZUNJlUEAPBVBBL0qaD0cK/jhX2MAwAwWAQS9IlnkwAA3IVAgj719WyS+DieTQIAcC02taJPxz6bpKCkSsnxkRoZE6bq+iMaGRNmdnkAAB/CDAlOqOvZJHPOTVF13RFZJB2ubdGajaUq3V+rVvvQX7oHAAAzJOi3KemjtPLV7dwGDABwOWZI0G/7DzVwGzAAwBAEEvQbtwEDAIxCIEG/cRswAMAoBBL0W1+3AcdFhZhUEQDAV7CpFf127G3AhaWHNX5stEZEhait3aGi8hp9sbeaF/ABAAaFQIIB6boNOHlMpCSpvrFVT/9tB3feAACGhCUbDMmh2mbuvAEADBmBBEPCnTcAAFcgkGBI+rrzpq9xAAB6QyDBkPR2582Cb47X6BFhKqmodXrEvMViMalKAICnY1MrhuT4O29OHRet+FPCtfLVz3tsdL1p3iRCCQCgVwQSDNnxd94U76vpdaPr3GmpioqKMqFCAICnY8kGLrdrT3WPMWtQgKrrWxQbG6vAQG4HBgA4I5DA5Y7f0JqaEKnVd31TkWE2bcivUsWhI2q1t5tUHQDAE7FkA5fr2ui6bku5rEEBunvhWXp+bQEPTwMA9IlAApc7dqNrTX2LauqP9Lqn5NJpqUo6uu8EAODfCCQwRNdGV0las7G0x+epCZGKiw5VSUWtCssO8w4cAPBzBBIY7vg9JV3LOE/9NZ9lHACAJDa1wg2Of3jaWRNH61BNz3fgfLi1QjX1Le4uDwDgAZghgeGO3VOys6RKZ08ao035Xzkdk5oQqbsXnqXDdc3aUniQJRwA8DMEEriFzRqoMbHDZHMEa3hIoNMyDnfiAABYsoHbtLe368CBA7Lb7U7LOGdNHK2Dhxt7LOGUfVWnjg5Hj3fiAAB8DzMkMMWxyzit9nYVldc4fd41a7LqtZ7vxGHWBAB8DzMkME3XrcETxsUoIyXW6bO+Zk3Y+AoAvolAAo9w/J04o2PDVLyv1umYrkfQ1za2sIQDAD6GJRt4hGOXcApLD2tK+kg1HrF3f97XxtcrvjVe86ePV8XXDTxgDQC8GIEEHqNrCafrCa+t9vbud+L0toRjDQrQzLOStfKV7U7j38odq5vmTVZQEBOAAOAtCCTwWCfb+NpbSElNiNR3Z6Zrz4E67dpTzYwJAHgJAgk82rHvxDk+VIyODXMKKSzrAID3IpDAa3RtfF23pTNsfFXVqNyJo7o/H8iyDiEFADwLgQRe4/iNrxnJMRp9TEg5fsZEGlhIYe8JAJiHQAKvcvzGV0ndIaWmvkUR4Tan4/sbUth7AgDmIpDA6x27z+TYO3Oknss6EntPAMATEUjgU062rCMNfu9JakKkfn7jVO0joACAyxFI4HNOtKwz2L0nXbMoz/xt5wlnUbLHj9CIqFBmVQBggAgk8AtD3XvSn1kUa1CAVt/1Te7oAYBBIJDAbw1k70l/ZlGGsvSTEBcuh8RyEAC/RSABdPK9J19VNeqM9FOcvnN8SBns0o81KEDP3fMtly4HBQYGatSoUQoMJNAA8A4EEuCoEy3rfFFWrZQxkSecRelPaOltFuWsiaO17+sGlywHdYeWQ80qKG1Rho4o4ZQgZloAeDwCCXACx4eUE82ifLzjK33/komaPiWxOzT0Z+nHVctBAw4tLBkB8CAEEmAATnYHz5FWuxZfNlmXnt//pR9XLQcNNrT0tmTU330uBBkArkIgAYaot5AykKWfj3d8pR/MyxryctBgQ8vxS0b93ecy2CDTnxmawR5jRhhivw7gGoYFkuLiYt1///3aunWrwsLCNHfuXN12222y2Wwn/zLgY0609JOeHKMQW9CQl4MGG1oGu89lMEGmPzM0gz1GGvzy1JAC0TH7dRJPCfLaYAWYzZBAUltbq2uvvVZJSUl6/PHHdfDgQT344IM6cuSI7rnnHiMuCXiV3mZVpL5nVvqzHDTY0HL8WH/3uQwmyBh5zGCXp9wZmgYbrNw90zSYY4byvf4GMGajfJshgeSll15SY2OjnnjiCUVFRUmS2tvbdd9992nRokUaOXKkEZcFfM5AloOODy07S6o0MSX2pKHl+CWj/u5zGUyQMfKYwSxP9TbmacGqa8yTQpOrgp00wGW+Ic5GmR3avKHGroBosVjkboYEkg8//FBTp07tDiOSNGvWLN17773auHGj5s2bZ8RlAb/UW2gZEztMNkewRsQOU/Bxz1g5fqbl+CWj/u5zGUyQMfKYwQSbwX7PncGqa8yTQpOrgp07l/mMPLev1NglLydRN82b5PZQEmDESUtKSpSSkuI0FhERobi4OJWUlBhxSQDHaG9v14EDB9Te3i7pP6Fl9jnJio8bruBj/pw8JlJWa2D3MRednaRhwZ0B5bEfXaCb5k3Sr384rTu09DU2KXVEd5Dp8vGOr5RwSnj32Mc7vtLImDBNn+L6Y76qatSpSTFOf05LjHLqS3/GzD7G7EDkzmDnDaHJn2rssm5LufYfanSaVHAHQ2ZI6urqFBER0WM8MjJStbW1gzqnw+FQU1PTUEvrobm52emfMA69dh9X9XpM7DDFj4iXw+GQ3d5ykrExkjp007xJmjstVQVH97kMswU6jR07Q+PKYzL7saemPzM9/fmekcccv++na8xdM02DOWaw3/OG0ORPNR6roLRK2UnBQ/47xOFw9HumxWtu+7Xb7SosLDTs/GVlZYadG87otfuY0WuLxaKoqChlJwWr5UiVduQXS9J/xuorVbCvVpGRkS4/pqRov26cm6lvn5Okwj01ampu0eL5k3TJeckqLKtW+rgoWYMsWnTpxO5jehvrz/eMOiZjXJTiRw5X3pRErTMhEA3mmMF+zxtCkz/VeKxTx0appaVGBw4c6PXzgejv3bUWh8PhGPLVjjN16lRddtll+tGPfuQ0ft5552nu3LlatmzZgM6Xn58vh8OhtLQ0V5YpqfPfIMvKypSUlKSQkBCXnx//Qa/dx997HRgYKIvFIofDofb29h5/7u2YwX6vvb29e1a46+6PwZzn2D93dHQoINCq/Ycau2eI4uPC5HCoe6xrg+L+ygaPOGYw39tVVq0Lz0jQk69/3h1suvY1PPv3nVq35T97H55fW9Bj78NAjjHy3L5SY5euPSQ7d3yucePGDenvkKKiIlksFmVlZZ30WEMCyZVXXqmoqCitWLGie6y+vl45OTl64IEHBrypNT8/X5L69V9ooJqamlRYWKj09HSFhoa6/Pz4D3rtPvTafej10LXa21VR2dC9ybrrzo+uscnHBJudJVXKTIk94TF9nedk5/aEYzzh+vFx4Wpva1VBQcGQf64H8vvbkEDyf//3f3ryySf1wQcfdO8lefnll3Xvvfdq/fr1A77tl0DiG+i1+9Br96HX7tPS0qJDhw5pxIgRCg4ONrscn+aqn+uB/P425C6bBQsWKCwsTDfffLM2bNigV199VQ899JAWLFjAM0gAAINy/N1j8C2GBJLIyEj9/ve/V2BgoG6++Wb9+te/1mWXXably5cbcTkAAODlDLvLJjU1Vc8995xRpwcAAD7EkBkSAACAgSCQAAAA0xFIAACA6QgkAADAdAQSAABgOgIJAAAwHYEEAACYjkACAABMZ8i7bFzts88+k8Ph6PcrjAfC4XDIbrfLarXKYrG4/Pz4D3rtPvTafei1+9Br93FVr1tbW2WxWHT66aef9FjDntTqSkb+4FksFkOCDnqi1+5Dr92HXrsPvXYfV/XaYrH0+3e4V8yQAAAA38YeEgAAYDoCCQAAMB2BBAAAmI5AAgAATEcgAQAApiOQAAAA0xFIAACA6QgkAADAdAQSAABgOgIJAAAwHYEEAACYzm8DSXFxsb73ve8pOztb55xzjh566CG1traaXZbX+8c//qGbbrpJ06ZNU3Z2tubOnatXXnlFx78y6eWXX9bMmTOVlZWlSy65ROvXrzepYt/Q2NioadOmacKECcrPz3f6jF67zuuvv65LL71UWVlZys3N1fXXX68jR450f/7+++/rkksuUVZWlmbOnKlXX33VxGq917p16/Sd73xHp512ms4991z98Ic/VHl5eY/j+NkemD179uiee+7R3LlzlZGRoYsvvrjX4/rT1/r6et11110688wzddppp2nJkiX6+uuvh1SfXwaS2tpaXXvttbLb7Xr88ce1dOlS/eUvf9GDDz5odmle77nnnlNISIiWL1+uVatWadq0abr77ru1YsWK7mPWrFmju+++W7NmzdLq1auVnZ2tW265Rdu2bTOvcC+3cuVKtbe39xin166zatUq/eIXv9Ds2bP1zDPP6Oc//7kSEhK6+/7JJ5/olltuUXZ2tlavXq1Zs2bpJz/5id566y2TK/cumzdv1i233KK0tDStWLFCd911l3bt2qWFCxc6hT9+tgfuyy+/1AcffKBx48YpNTW112P629fbbrtNGzdu1M9+9jM9/PDDKi0t1Q033KC2trbBF+jwQ08++aQjOzvbUV1d3T320ksvOdLT0x0HDhwwrzAfUFVV1WPspz/9qeP00093tLe3OxwOh+Nb3/qW4/bbb3c65vLLL3dcf/31bqnR1xQVFTmys7MdL774omP8+PGOzz//vPszeu0axcXFjoyMDMc///nPPo9ZuHCh4/LLL3cau/322x2zZs0yujyfcvfddzumT5/u6Ojo6B7btGmTY/z48Y4tW7Z0j/GzPXBdfwc7HA7HHXfc4ZgzZ06PY/rT188++8wxfvx4x0cffdQ9Vlxc7JgwYYJjzZo1g67PL2dIPvzwQ02dOlVRUVHdY7NmzVJHR4c2btxoXmE+ICYmpsdYenq6Ghoa1NTUpPLycpWVlWnWrFlOx8yePVubNm1i2WwQ7r//fi1YsEDJyclO4/TadV577TUlJCTo/PPP7/Xz1tZWbd68WRdddJHT+OzZs1VcXKx9+/a5o0yf0NbWprCwMFkslu6x4cOHS1L30i8/24MTEHDiX/n97euHH36oiIgInXPOOd3HpKSkKD09XR9++OHg6xv0N71YSUmJUlJSnMYiIiIUFxenkpISk6ryXZ9++qlGjhyp8PDw7v4e/8szNTVVdru913Vi9O2tt97S7t27dfPNN/f4jF67zvbt2zV+/HitXLlSU6dO1cSJE7VgwQJt375dkrR3717Z7fYef690TYvz90r/zZs3T8XFxXrhhRdUX1+v8vJy/eY3v1FGRoZOP/10SfxsG6W/fS0pKVFycrJTaJQ6Q8lQftb9MpDU1dUpIiKix3hkZKRqa2tNqMh3ffLJJ1q7dq0WLlwoSd39Pb7/XX+m//3X3NysBx98UEuXLlV4eHiPz+m161RWVmrDhg164403dO+992rFihWyWCxauHChqqqq6LULTZkyRU888YR+/etfa8qUKZoxY4aqqqq0evVqBQYGSuJn2yj97WtdXV33rNWxhvo71C8DCdzjwIEDWrp0qXJzc3XNNdeYXY7PWbVqlWJjYzV//nyzS/F5DodDTU1NevTRR3XRRRfp/PPP16pVq+RwOPTHP/7R7PJ8ymeffaYf//jH+u///m/9/ve/16OPPqqOjg7deOONTpta4Xv8MpBERESovr6+x3htba0iIyNNqMj31NXV6YYbblBUVJQef/zx7rXLrv4e3/+6ujqnz3FiFRUVevbZZ7VkyRLV19errq5OTU1NkqSmpiY1NjbSaxeKiIhQVFSUTj311O6xqKgoZWRkqKioiF670P3336+zzjpLy5cv11lnnaWLLrpITz31lAoKCvTGG29I4u8Ro/S3rxEREWpoaOjx/aH+DvXLQNLbOld9fb0qKyt7rAFj4I4cOaJFixapvr5eTz/9tNPUXld/j+9/SUmJrFarEhMT3Vqrt9q3b5/sdrtuvPFG5eTkKCcnRz/4wQ8kSddcc42+973v0WsXSktL6/OzlpYWjR07VlartddeS+LvlQEoLi52Cn6SNGrUKEVHR2vv3r2S+HvEKP3ta0pKikpLS3s8X6q0tHRIP+t+GUimTZumf/3rX92pT+rcHBgQEOC0axgD19bWpttuu00lJSV6+umnNXLkSKfPExMTlZSU1OPZDGvXrtXUqVNls9ncWa7XSk9P1/PPP+/0f3feeack6b777tO9995Lr13owgsvVE1NjQoLC7vHqqurtXPnTmVmZspmsyk3N1dvv/220/fWrl2r1NRUJSQkuLtkrzVmzBgVFBQ4jVVUVKi6ulrx8fGS+HvEKP3t67Rp01RbW6tNmzZ1H1NaWqqCggJNmzZt0NcPGvQ3vdiCBQv0hz/8QTfffLMWLVqkgwcP6qGHHtKCBQt6/ALFwNx3331av369li9froaGBqeH6WRkZMhms+nWW2/VsmXLNHbsWOXm5mrt2rX6/PPPWYsfgIiICOXm5vb6WWZmpjIzMyWJXrvIjBkzlJWVpSVLlmjp0qUKDg7WU089JZvNpu9+97uSpJtuuknXXHONfvazn2nWrFnavHmz3nzzTf32t781uXrvsmDBAj3wwAO6//77NX36dNXU1HTvlzr2dlR+tgeuublZH3zwgaTOkNfQ0NAdPs4880zFxMT0q69dT9C96667dMcddyg4OFi//e1vNWHCBH3rW98adH0Wx/FzLn6iuLhYv/jFL7R161aFhYVp7ty5Wrp0Kcl6iKZPn66KiopeP1u3bl33vym+/PLLWr16tfbv36/k5GTdfvvtuvDCC91Zqs/ZvHmzrrnmGr3yyivKysrqHqfXrnH48GH98pe/1Pr162W32zVlyhTdeeedTss569at0yOPPKLS0lKNGTNGN954oy677DITq/Y+DodDL730kl588UWVl5crLCxM2dnZWrp0aY+ni/KzPTD79u1TXl5er589//zz3f+S05++1tfX65e//KXeffddtbW16dxzz9VPf/rTIf1Lvd8GEgAA4Dn8cg8JAADwLAQSAABgOgIJAAAwHYEEAACYjkACAABMRyABAACmI5AAAADTEUgAAIDpCCQAAMB0BBIAAGA6AgkAADAdgQQAAJju/wOzWb8fGnYAQwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.lineplot(x=range(len(train_losses)), y=train_losses, marker='o', markersize=5)\n",
    "plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Some Examples\n",
    "\n",
    "Generate using greedy decoding. You will learn more about it in class later in the semester.\n",
    "\n",
    "Here is a [link for a checkpoint](https://drive.google.com/file/d/130dDwMBJGhvSEFQdHkxaU-R5IjlSbO2j/view?usp=sharing) that you can use for prediction and attention weight visualization.\n",
    "It was trained using the following configuration\n",
    "```\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "hidden_dim = 2048\n",
    "num_layers = 4\n",
    "vocab_size = 32000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = Transformer(\n",
    "    src_vocab_size=hf_tokenizer.vocab_size, \n",
    "    tgt_vocab_size=hf_tokenizer.vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "state_dict = torch.load(\"transformer_model.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "src_val = [dataset['train'][i]['translation']['fr'] for i in range(5)]\n",
    "tgt_val = [dataset['train'][i]['translation']['en'] for i in range(5)]\n",
    "\n",
    "src_tokens_val = hf_tokenizer(src_val, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        # Prepare inputs\n",
    "        src = src_tokens_val['input_ids'][i].unsqueeze(0).to(device)\n",
    "        tgt = torch.tensor([[hf_tokenizer.bos_token_id]], dtype=torch.long, device=device)\n",
    "\n",
    "        # Greedy decode up to max_len\n",
    "        for step in range(max_len):\n",
    "            # Create tgt padding mask\n",
    "            tgt_mask = torch.ones_like(tgt).bool().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, _ = model(src, tgt, tgt_mask=tgt_mask)\n",
    "\n",
    "            # Get the most probable token at the current step\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1)\n",
    "\n",
    "            # Append\n",
    "            tgt = torch.cat([tgt, next_token.unsqueeze(-1)], dim=1)\n",
    "\n",
    "            # End conditions\n",
    "            if next_token.item() in [hf_tokenizer.eos_token_id, hf_tokenizer.pad_token_id]:\n",
    "                break\n",
    "\n",
    "        # Decode\n",
    "        translation = hf_tokenizer.decode(tgt[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Source:      {src_val[i]}\")\n",
    "        print(f\"  Translation: {translation}\")\n",
    "        print(f\"  Target:      {tgt_val[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLXazZQx5roi"
   },
   "source": [
    "## Analyze Cross-Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "900FaGrbMCms"
   },
   "source": [
    "- Choose one sample sentence, output attention weights for each token using heatmap\n",
    "- Which pairs of the token have the greatest attention weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T15:18:26.595566Z",
     "iopub.status.busy": "2024-02-22T15:18:26.594626Z",
     "iopub.status.idle": "2024-02-22T15:18:27.145253Z",
     "shell.execute_reply": "2024-02-22T15:18:27.144212Z",
     "shell.execute_reply.started": "2024-02-22T15:18:26.595530Z"
    },
    "id": "QQMMFUipeVkd",
    "outputId": "f5822cd3-9fdf-4af5-9333-1eef3fad1c4e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_cross_attention(\n",
    "    attn_weights, \n",
    "    source_tokens, \n",
    "    target_tokens, \n",
    "    batch_idx=0, \n",
    "    head_idx=0, \n",
    "    title=\"Cross-Attention\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize cross-attention weights for a given batch and head.\n",
    "\n",
    "    Args:\n",
    "        attn_weights: Tensor of shape [batch_size, num_heads, tgt_len, src_len]\n",
    "            Cross-attention weights from your Transformer decoder, \n",
    "            typically returned alongside logits in a (logits, attn_weights) tuple.\n",
    "        source_tokens: List of source tokens (strings) for the batch_idx sample.\n",
    "        target_tokens: List of target tokens (strings) for the batch_idx sample.\n",
    "        batch_idx: Which batch element to visualize (default=0).\n",
    "        head_idx: Which attention head to visualize (default=0).\n",
    "        title: Title for the plot.\n",
    "\n",
    "    Example Usage:\n",
    "        # Suppose attn_weights has shape [batch_size, num_heads, tgt_len, src_len]\n",
    "        # and you have the corresponding token lists for the source and target:\n",
    "        visualize_cross_attention(attn_weights, src_tokens, tgt_tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Extract the attention for the specified batch & head\n",
    "    #    shape: [tgt_len, src_len]\n",
    "    attn = attn_weights[batch_idx, head_idx].detach().cpu().numpy()\n",
    "\n",
    "    tgt_len, src_len = attn.shape\n",
    "\n",
    "    # 2) Plot the heatmap\n",
    "    plt.figure(figsize=(min(12, 1 + 0.5 * src_len), min(6, 1 + 0.5 * tgt_len)))\n",
    "    sns.heatmap(attn, \n",
    "                vmin=0.0, vmax=1.0, \n",
    "                cmap=\"Blues\", \n",
    "                xticklabels=source_tokens, \n",
    "                yticklabels=target_tokens, \n",
    "                cbar=True)\n",
    "\n",
    "    plt.title(f\"{title} (batch={batch_idx}, head={head_idx})\")\n",
    "    plt.xlabel(\"Source Tokens\")\n",
    "    plt.ylabel(\"Target Tokens\")\n",
    "\n",
    "    # Rotate the x-axis labels if tokens are long\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T15:18:40.105997Z",
     "iopub.status.busy": "2024-02-22T15:18:40.105643Z",
     "iopub.status.idle": "2024-02-22T15:18:40.181096Z",
     "shell.execute_reply": "2024-02-22T15:18:40.180179Z",
     "shell.execute_reply.started": "2024-02-22T15:18:40.105972Z"
    },
    "id": "HOxwg2xw52rG",
    "outputId": "f4b013bd-85e3-44ba-eba4-6656db8963cf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "src_val = \"Le renard brun rapide saute par-dessus le chien paresseux.\"\n",
    "tgt_val = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the source and target\n",
    "src_tokens_val = hf_tokenizer(src_val, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "tgt_tokens_val = hf_tokenizer(tgt_val, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "\n",
    "print(src_tokens_val[\"input_ids\"].shape)\n",
    "print(tgt_tokens_val[\"input_ids\"].shape)\n",
    "\n",
    "# Run the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits, attn_weights = model(\n",
    "        src=src_tokens_val['input_ids'],\n",
    "        tgt=tgt_tokens_val['input_ids'],\n",
    "        src_mask=src_tokens_val['attention_mask'],\n",
    "        tgt_mask=tgt_tokens_val['attention_mask']\n",
    "    )\n",
    "\n",
    "# Decode the source and target sequences\n",
    "src_tokens = hf_tokenizer.convert_ids_to_tokens(src_tokens_val[\"input_ids\"][0])\n",
    "tgt_tokens = hf_tokenizer.convert_ids_to_tokens(tgt_tokens_val[\"input_ids\"][0])\n",
    "\n",
    "# Visualize the attention weights\n",
    "for i in range(num_heads):\n",
    "    visualize_cross_attention(\n",
    "        attn_weights=attn_weights,\n",
    "        source_tokens=src_tokens,\n",
    "        target_tokens=tgt_tokens,\n",
    "        batch_idx=0,\n",
    "        head_idx=i\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpXzr36fLUtI"
   },
   "source": [
    "## Congrats! You can now train a simple machine translator by your own ;)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4480106,
     "sourceId": 7679312,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8ced340a52f9326f5856e1d63a73f97bd9f0a225610b549ff7b502d766a19ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
